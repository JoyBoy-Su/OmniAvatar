# Audio-Video Joint Generation Model Configuration
# Model: Unified DiT with Dual Decoders

model_name: "AVGen-DiT-3B"
version: "v1.0"

# ========================
# DiT Backbone Configuration
# ========================
dit:
  # Architecture
  input_size: [64, 64]  # Latent space size
  patch_size: 2
  in_channels: 8  # 4 for audio + 4 for video latents
  hidden_size: 2048
  depth: 24  # Number of transformer blocks
  num_heads: 16
  mlp_ratio: 4.0
  
  # Dropout
  dropout: 0.0
  attention_dropout: 0.0
  
  # Position Encoding
  pos_embed_type: "learnable"  # or "rope", "sincos"
  
  # Normalization
  norm_type: "layer_norm"
  norm_eps: 1e-6
  
  # Activation
  activation: "gelu"
  
  # AdaLN
  use_adaln: true
  adaln_single: false
  
  # Attention
  use_flash_attention: true
  use_xformers: true
  
  # Gradient Checkpointing
  gradient_checkpointing: true
  checkpointing_every_n_blocks: 3

# ========================
# Audio Decoder Configuration
# ========================
audio_decoder:
  # Architecture
  type: "transformer_decoder"  # or "unet", "conv"
  
  # Input/Output
  latent_dim: 2048
  output_channels: 80  # Mel-spectrogram bins
  
  # Decoder Blocks
  num_layers: 8
  hidden_size: 1024
  num_heads: 8
  mlp_ratio: 4.0
  
  # Temporal Configuration
  sequence_length: 256  # Number of time frames
  
  # Upsampling
  upsampling_layers: [2, 2, 2]  # Total 8x upsampling
  
  # Audio Specific
  sample_rate: 16000
  hop_length: 256
  n_fft: 1024
  n_mels: 80
  
  # Vocoder
  vocoder:
    type: "hifigan"  # or "bigvgan", "encodec"
    pretrained: "nvidia/bigvgan_v2_22khz_80band"
    freeze_vocoder: true

# ========================
# Video Decoder Configuration
# ========================
video_decoder:
  # Architecture
  type: "3d_conv_decoder"  # or "transformer_decoder"
  
  # Input/Output
  latent_dim: 2048
  output_channels: 4  # VAE latent channels
  
  # Decoder Blocks
  num_layers: 6
  hidden_channels: [512, 256, 128]
  
  # Temporal Configuration
  num_frames: 16  # Output video frames
  fps: 8  # Frames per second
  
  # Spatial Configuration
  resolution: 512  # Output resolution (512x512)
  
  # 3D Convolution
  use_3d_conv: true
  temporal_kernel_size: 3
  
  # Upsampling
  upsampling_factors: [2, 2, 2]  # 8x spatial upsampling
  
  # VAE
  vae:
    type: "sd_vae"  # Stable Diffusion VAE
    pretrained: "stabilityai/sd-vae-ft-mse"
    freeze_vae: true
    scaling_factor: 0.18215

# ========================
# Encoder Configuration (frozen)
# ========================
encoders:
  # Text Encoder
  text:
    model: "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
    max_length: 77
    freeze: true
    output_dim: 1024
  
  # Audio Encoder (for conditioning/alignment)
  audio:
    model: "facebook/wav2vec2-large-robust"
    freeze: true
    output_dim: 1024
    
  # Video Encoder (for conditioning/alignment)
  video:
    model: "MCG-NJU/videomae-large"
    freeze: true
    output_dim: 1024
  
  # Cross-Modal Encoder
  cross_modal:
    model: "LanguageBind/LanguageBind_Audio_FT"  # or ImageBind
    freeze: true
    use_for_alignment: true

# ========================
# Temporal Alignment Module
# ========================
temporal_alignment:
  enabled: true
  
  # Architecture
  hidden_dim: 1024
  num_layers: 4
  num_heads: 8
  
  # Cross-Attention
  use_cross_attention: true
  attention_type: "bidirectional"  # audio<->video
  
  # Temporal Modeling
  use_temporal_conv: true
  temporal_kernel_size: 5
  
  # Loss Weight
  alignment_loss_weight: 0.1

# ========================
# Diffusion Configuration
# ========================
diffusion:
  # Scheduler
  scheduler_type: "ddpm"  # or "ddim", "pndm", "dpm_solver"
  
  # Timesteps
  num_train_timesteps: 1000
  num_inference_steps: 50
  
  # Beta Schedule
  beta_schedule: "scaled_linear"  # or "linear", "cosine"
  beta_start: 0.0001
  beta_end: 0.02
  
  # Prediction Type
  prediction_type: "epsilon"  # or "v_prediction", "sample"
  
  # Sampling
  use_karras_sigmas: false
  clip_sample: false
  clip_sample_range: 1.0
  
  # CFG (Classifier-Free Guidance)
  guidance_scale: 7.5
  conditioning_dropout_prob: 0.1

# ========================
# Training Configuration
# ========================
training:
  # Optimization
  learning_rate: 1e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  
  # Learning Rate Schedule
  lr_scheduler: "cosine"  # or "constant", "linear", "polynomial"
  lr_warmup_steps: 10000
  lr_warmup_type: "linear"
  min_lr: 1e-6
  
  # Batch Size
  train_batch_size: 16  # Per GPU
  gradient_accumulation_steps: 4  # Effective batch = 16 * 4 * num_gpus
  
  # Mixed Precision
  mixed_precision: "bf16"  # or "fp16", "no"
  
  # EMA (Exponential Moving Average)
  use_ema: true
  ema_decay: 0.9999
  ema_update_every: 10
  
  # Checkpointing
  save_steps: 5000
  save_total_limit: 5
  resume_from_checkpoint: null
  
  # Logging
  logging_steps: 100
  eval_steps: 2000
  
  # Training Stages
  stages:
    stage1:
      name: "single_modality_pretraining"
      epochs: 1
      data: "audio_only+video_only"
      learning_rate: 1e-4
      
    stage2:
      name: "joint_training"
      epochs: 2
      data: "audio_video_pairs"
      learning_rate: 5e-5
      alignment_loss_weight: 0.2
      
    stage3:
      name: "fine_tuning"
      epochs: 3
      data: "high_quality_pairs"
      learning_rate: 1e-5
      alignment_loss_weight: 0.5

# ========================
# Loss Configuration
# ========================
loss:
  # Main Losses
  diffusion_loss:
    type: "mse"  # or "l1", "huber"
    weight: 1.0
  
  # Alignment Loss
  alignment_loss:
    type: "contrastive"  # or "mse", "cosine"
    weight: 0.1
    temperature: 0.07
    
  # Perceptual Loss
  perceptual_loss:
    enabled: true
    weight: 0.05
    audio_model: "laion/clap-htsat-unfused"
    video_model: "openai/clip-vit-large-patch14"
  
  # Temporal Consistency Loss
  temporal_loss:
    enabled: true
    weight: 0.02
    type: "lpips"  # for video
  
  # Adversarial Loss (optional)
  adversarial_loss:
    enabled: false
    weight: 0.01
    discriminator_lr: 4e-4

# ========================
# Data Configuration
# ========================
data:
  # Dataset
  train_data_dir: "/path/to/train/data"
  val_data_dir: "/path/to/val/data"
  
  # Format
  data_format: "webdataset"
  
  # Preprocessing
  audio_sample_rate: 16000
  audio_duration: 10  # seconds
  video_fps: 8
  video_num_frames: 16
  video_resolution: 512
  
  # Augmentation
  augmentation:
    audio:
      time_stretch: true
      pitch_shift: true
      add_noise: true
      volume_norm: true
      
    video:
      random_crop: true
      horizontal_flip: true
      color_jitter: true
      
    joint:
      temporal_crop: true
      speed_change: false
  
  # Data Loading
  num_workers: 8
  prefetch_factor: 4
  pin_memory: true
  persistent_workers: true
  
  # Filtering
  min_duration: 3
  max_duration: 60
  min_resolution: 256
  min_audio_snr: 10

# ========================
# Evaluation Configuration
# ========================
evaluation:
  # Metrics
  metrics:
    - "fad"  # Fréchet Audio Distance
    - "fvd"  # Fréchet Video Distance
    - "clap_score"  # Audio-text alignment
    - "clip_score"  # Video-text alignment
    - "ava_score"  # Audio-visual alignment
    - "temporal_consistency"
  
  # Generation Settings
  num_samples: 1000
  batch_size: 8
  guidance_scale: 7.5
  num_inference_steps: 50
  
  # Human Evaluation
  human_eval:
    enabled: false
    num_samples: 100
    criteria: ["audio_quality", "video_quality", "sync", "semantics"]

# ========================
# Inference Configuration
# ========================
inference:
  # Model
  model_path: "checkpoints/best_model"
  use_ema: true
  
  # Generation
  guidance_scale: 7.5
  num_inference_steps: 50
  
  # Output
  output_format: "mp4"
  output_resolution: 512
  output_fps: 24
  audio_sample_rate: 44100
  
  # Optimization
  enable_xformers: true
  enable_attention_slicing: true
  enable_vae_slicing: true
  enable_cpu_offload: false

# ========================
# Distributed Training Configuration
# ========================
distributed:
  # Backend
  backend: "nccl"
  
  # Strategy
  strategy: "ddp"  # or "deepspeed", "fsdp"
  
  # DeepSpeed
  deepspeed:
    enabled: true
    config_file: "configs/deepspeed_config.json"
    zero_stage: 2
    
  # FSDP
  fsdp:
    enabled: false
    sharding_strategy: "FULL_SHARD"
    
  # Communication
  gradient_as_bucket_view: true
  find_unused_parameters: false

# ========================
# Monitoring Configuration
# ========================
monitoring:
  # Experiment Tracking
  use_wandb: true
  wandb_project: "audio-video-generation"
  wandb_entity: "research-team"
  wandb_run_name: null  # Auto-generated
  
  # Tensorboard
  use_tensorboard: true
  tensorboard_dir: "logs/tensorboard"
  
  # Logging
  log_level: "INFO"
  log_to_file: true
  log_file: "logs/training.log"

# ========================
# Hardware Configuration
# ========================
hardware:
  # GPU
  num_gpus: 8
  gpu_type: "A100-80GB"
  
  # CPU
  num_workers: 64
  
  # Memory
  max_memory_per_gpu: "76GB"
  
  # Storage
  cache_dir: "/mnt/cache"
  output_dir: "outputs"

