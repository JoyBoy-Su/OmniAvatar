# éŸ³è§†é¢‘è”åˆç”Ÿæˆ (Audio-Video Joint Generation) é¡¹ç›®

è¿™æ˜¯ä¸€ä¸ªåŸºäºDiffusion Transformer (DiT)çš„éŸ³è§†é¢‘è”åˆç”Ÿæˆç ”ç©¶é¡¹ç›®ã€‚æœ¬é¡¹ç›®å®ç°äº†ä»æ–‡æœ¬/éŸ³é¢‘/è§†é¢‘æ¡ä»¶ç”Ÿæˆé«˜è´¨é‡ã€åŒæ­¥çš„éŸ³è§†é¢‘å†…å®¹ã€‚

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
- [æ¨¡å‹è®­ç»ƒ](#æ¨¡å‹è®­ç»ƒ)
- [æ¨ç†ä½¿ç”¨](#æ¨ç†ä½¿ç”¨)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

### æ ¸å¿ƒç‰¹æ€§

- **ç»Ÿä¸€DiTæ¶æ„**: ä½¿ç”¨å•ä¸€çš„Diffusion Transformerå¤„ç†éŸ³è§†é¢‘è”åˆç”Ÿæˆ
- **åŒè§£ç å™¨è®¾è®¡**: ç‹¬ç«‹çš„éŸ³é¢‘å’Œè§†é¢‘è§£ç å™¨ï¼Œä¿è¯å„æ¨¡æ€ç”Ÿæˆè´¨é‡
- **æ—¶åºå¯¹é½æ¨¡å—**: ç¡®ä¿ç”Ÿæˆçš„éŸ³é¢‘å’Œè§†é¢‘åœ¨æ—¶åºä¸Šå®Œç¾åŒæ­¥
- **å¤§è§„æ¨¡è®­ç»ƒ**: æ”¯æŒ50M+æ ·æœ¬ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ç­‰
- **å¤šç§æ¡ä»¶è¾“å…¥**: æ”¯æŒæ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ä½œä¸ºæ¡ä»¶è¾“å…¥

### æŠ€æœ¯æ ˆ

```
æ ¸å¿ƒæ¡†æ¶:   PyTorch 2.1+, Hugging Face Diffusers
åˆ†å¸ƒå¼:     DeepSpeed, FSDP, Accelerate
ä¼˜åŒ–:       FlashAttention-2, xFormers, torch.compile
æ•°æ®:       WebDataset, ffmpeg, librosa
ç›‘æ§:       Weights & Biases, TensorBoard
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-org/audio-video-generation.git
cd audio-video-generation

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n avgen python=3.10
conda activate avgen

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å®‰è£…é¢å¤–ä¾èµ–
pip install flash-attn --no-build-isolation
pip install git+https://github.com/facebookresearch/xformers.git
```

### 2. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹

```bash
# ä¸‹è½½é¢„è®­ç»ƒçš„ç¼–ç å™¨å’ŒVAE
python scripts/download_pretrained.py

# æˆ–æ‰‹åŠ¨ä¸‹è½½
# CLIP: openai/clip-vit-large-patch14
# Wav2Vec2: facebook/wav2vec2-large-robust
# SD-VAE: stabilityai/sd-vae-ft-mse
# HiFi-GAN: nvidia/bigvgan_v2_22khz_80band
```

### 3. å‡†å¤‡ç¤ºä¾‹æ•°æ®

```bash
# ä¸‹è½½ç¤ºä¾‹æ•°æ®é›† (1K samples)
bash scripts/download_demo_data.sh

# æˆ–ä½¿ç”¨è‡ªå·±çš„æ•°æ®
python scripts/data_preprocessing.py \
    --input-dir /path/to/your/videos \
    --output-dir /data/processed \
    --mode all
```

### 4. è®­ç»ƒæ¨¡å‹

```bash
# å•æœº8å¡è®­ç»ƒ
bash scripts/train_multi_gpu.sh

# æˆ–ä½¿ç”¨Pythonç›´æ¥è®­ç»ƒ
accelerate launch --multi_gpu --num_processes 8 \
    training/train.py \
    --config configs/model_config.yaml \
    --stage stage2
```

### 5. æ¨ç†ç”Ÿæˆ

```python
from pipelines import AudioVideoGenerationPipeline

# åŠ è½½æ¨¡å‹
pipeline = AudioVideoGenerationPipeline.from_pretrained(
    "checkpoints/best_model",
    torch_dtype=torch.bfloat16
)
pipeline = pipeline.to("cuda")

# æ–‡æœ¬åˆ°éŸ³è§†é¢‘ç”Ÿæˆ
prompt = "A person playing piano in a concert hall"
output = pipeline(
    prompt=prompt,
    num_inference_steps=50,
    guidance_scale=7.5,
    output_path="output.mp4"
)
```

---

## ğŸ“Š æ•°æ®å‡†å¤‡

### æ•°æ®çˆ¬å–

æˆ‘ä»¬æä¾›äº†å¤§è§„æ¨¡éŸ³è§†é¢‘æ•°æ®çˆ¬å–å·¥å…·ï¼Œæ”¯æŒYouTubeã€Bilibiliç­‰å¹³å°ã€‚

```bash
# 1. é…ç½®çˆ¬å–ä»»åŠ¡
# ç¼–è¾‘ crawler_config.jsonï¼Œæ·»åŠ æœç´¢å…³é”®è¯å’Œé¢‘é“

# 2. å¯åŠ¨Redis (ç”¨äºä»»åŠ¡é˜Ÿåˆ—)
docker run -d -p 6379:6379 redis:latest

# 3. æ”¶é›†URL
python scripts/data_crawler.py \
    --mode collect \
    --config crawler_config.json

# 4. å¯åŠ¨å¤šä¸ªçˆ¬å–worker
for i in {1..50}; do
    python scripts/data_crawler.py --mode worker --num-workers 1 &
done

# 5. åˆ†å‘ä¸‹è½½ä»»åŠ¡
python scripts/data_crawler.py --mode download

# 6. æŸ¥çœ‹ç»Ÿè®¡
python scripts/data_crawler.py --mode stats
```

### é¢„æœŸæ•°æ®è§„æ¨¡

| é˜¶æ®µ | æ•°æ®é‡ | æ—¶é•¿ | å­˜å‚¨ |
|------|--------|------|------|
| Stage 1 (å•æ¨¡æ€) | 20M | 55Kå°æ—¶ | 300TB |
| Stage 2 (è”åˆ) | 30M | 83Kå°æ—¶ | 450TB |
| Stage 3 (å¾®è°ƒ) | 2M | 8Kå°æ—¶ | 30TB |
| **æ€»è®¡** | **52M** | **146Kå°æ—¶** | **780TB** |

### æ•°æ®é¢„å¤„ç†

```bash
# è´¨é‡è¿‡æ»¤ + æ ¼å¼è½¬æ¢ + æ‰“åŒ…æˆWebDataset
python scripts/data_preprocessing.py \
    --input-dir /data/raw \
    --output-dir /data/processed \
    --num-workers 32 \
    --samples-per-shard 1000 \
    --mode all
```

é¢„å¤„ç†åŒ…æ‹¬:
- âœ… åˆ†è¾¨ç‡æ£€æŸ¥ (â‰¥480p)
- âœ… å¸§ç‡æ£€æŸ¥ (â‰¥24fps)
- âœ… æ¨¡ç³Šåº¦æ£€æµ‹ (Laplacianæ–¹å·®)
- âœ… åœºæ™¯åˆ‡æ¢æ£€æµ‹ (é¿å…å¿«é€Ÿå‰ªè¾‘)
- âœ… éŸ³é¢‘ä¿¡å™ªæ¯” (SNR >10dB)
- âœ… é™éŸ³æ¯”ä¾‹ (<50%)
- âœ… å‰Šæ³¢æ£€æµ‹
- âœ… æ ¼å¼ç»Ÿä¸€ (512x512@8fps, 16kHz mono)

---

## ğŸ‹ï¸ æ¨¡å‹è®­ç»ƒ

### è®­ç»ƒé…ç½®

æ‰€æœ‰è®­ç»ƒé…ç½®åœ¨ `configs/model_config.yaml` ä¸­å®šä¹‰:

```yaml
# æ ¸å¿ƒæ¨¡å‹å‚æ•°
dit:
  hidden_size: 2048
  depth: 24
  num_heads: 16
  
# è®­ç»ƒé…ç½®
training:
  learning_rate: 1e-4
  train_batch_size: 16
  gradient_accumulation_steps: 4
  mixed_precision: "bf16"
  
# åˆ†å¸ƒå¼é…ç½®
distributed:
  strategy: "deepspeed"
  zero_stage: 2
```

### ä¸‰é˜¶æ®µè®­ç»ƒ

#### Stage 1: å•æ¨¡æ€é¢„è®­ç»ƒ (2-4å‘¨)

```bash
bash scripts/train_multi_gpu.sh
# ä¿®æ”¹è„šæœ¬ä¸­çš„ TRAINING_STAGE="stage1"
```

- æ•°æ®: 10MéŸ³é¢‘ + 10Mè§†é¢‘ (åˆ†åˆ«è®­ç»ƒæˆ–å¹¶è¡Œ)
- ç›®æ ‡: å»ºç«‹éŸ³é¢‘/è§†é¢‘ç”ŸæˆåŸºç¡€èƒ½åŠ›
- å­¦ä¹ ç‡: 1e-4

#### Stage 2: è”åˆè®­ç»ƒ (5-8å‘¨)

```bash
bash scripts/train_multi_gpu.sh
# TRAINING_STAGE="stage2"
```

- æ•°æ®: 30MéŸ³è§†é¢‘é…å¯¹æ•°æ®
- ç›®æ ‡: å­¦ä¹ éŸ³è§†é¢‘å¯¹é½å’Œè”åˆç”Ÿæˆ
- å­¦ä¹ ç‡: 5e-5
- å…³é”®: alignment lossæƒé‡é€æ­¥å¢åŠ 

#### Stage 3: é«˜è´¨é‡å¾®è°ƒ (1-2å‘¨)

```bash
bash scripts/train_multi_gpu.sh
# TRAINING_STAGE="stage3"
```

- æ•°æ®: 2Mé«˜è´¨é‡é…å¯¹æ•°æ®
- ç›®æ ‡: æå‡ç”Ÿæˆè´¨é‡å’Œç»†èŠ‚
- å­¦ä¹ ç‡: 1e-5
- æŠ€å·§: ä½¿ç”¨äººå·¥ç­›é€‰çš„é«˜è´¨é‡æ•°æ®

### åˆ†å¸ƒå¼è®­ç»ƒ

#### å•èŠ‚ç‚¹å¤šGPU (æ¨èç”¨äºå¼€å‘å’Œå°è§„æ¨¡å®éªŒ)

```bash
# ä½¿ç”¨Accelerate
accelerate launch --multi_gpu --num_processes 8 training/train.py

# ä½¿ç”¨DeepSpeed
deepspeed --num_gpus=8 training/train.py --deepspeed configs/deepspeed_config.json
```

#### å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ (å¤§è§„æ¨¡è®­ç»ƒ)

**èŠ‚ç‚¹0 (Master):**
```bash
export MASTER_ADDR=192.168.1.100
export MASTER_PORT=29500
export NODE_RANK=0
export WORLD_SIZE=4  # 4 nodes

deepspeed --num_gpus=8 --num_nodes=4 --node_rank=0 \
    training/train.py --deepspeed configs/deepspeed_config.json
```

**èŠ‚ç‚¹1-3 (Workers):**
```bash
export MASTER_ADDR=192.168.1.100
export MASTER_PORT=29500
export NODE_RANK=1  # 2, 3 for other nodes
export WORLD_SIZE=4

deepspeed --num_gpus=8 --num_nodes=4 --node_rank=$NODE_RANK \
    training/train.py --deepspeed configs/deepspeed_config.json
```

### ç›‘æ§è®­ç»ƒ

```bash
# ä½¿ç”¨Weights & Biases
export WANDB_API_KEY=your_api_key
# è‡ªåŠ¨è®°å½•åˆ° W&B

# æˆ–ä½¿ç”¨TensorBoard
tensorboard --logdir outputs/logs/
```

å…³é”®æŒ‡æ ‡:
- `loss/diffusion`: æ‰©æ•£æ¨¡å‹ä¸»æŸå¤±
- `loss/alignment`: éŸ³è§†é¢‘å¯¹é½æŸå¤±
- `metrics/fad`: éŸ³é¢‘è´¨é‡ (FrÃ©chet Audio Distance)
- `metrics/fvd`: è§†é¢‘è´¨é‡ (FrÃ©chet Video Distance)
- `metrics/ava`: éŸ³è§†é¢‘å¯¹é½åº¦

---

## ğŸ¨ æ¨ç†ä½¿ç”¨

### Python API

```python
import torch
from pipelines import AudioVideoGenerationPipeline

# åŠ è½½æ¨¡å‹
pipeline = AudioVideoGenerationPipeline.from_pretrained(
    "checkpoints/stage2_best",
    torch_dtype=torch.bfloat16,
    use_safetensors=True
)
pipeline.to("cuda")

# å¯ç”¨ä¼˜åŒ–
pipeline.enable_xformers_memory_efficient_attention()
pipeline.enable_vae_slicing()

# æ–‡æœ¬åˆ°éŸ³è§†é¢‘
output = pipeline(
    prompt="A violinist playing classical music in a grand hall",
    negative_prompt="low quality, blurry, distorted",
    num_inference_steps=50,
    guidance_scale=7.5,
    height=512,
    width=512,
    num_frames=16,
    fps=8,
    audio_duration=10.0,
    seed=42
)

# ä¿å­˜ç»“æœ
output.save("output.mp4")
```

### å‘½ä»¤è¡Œå·¥å…·

```bash
python scripts/generate.py \
    --checkpoint checkpoints/stage2_best \
    --prompt "A person playing piano" \
    --output output.mp4 \
    --steps 50 \
    --guidance-scale 7.5 \
    --seed 42
```

### æ‰¹é‡ç”Ÿæˆ

```bash
# ä»æ–‡ä»¶è¯»å–å¤šä¸ªprompt
python scripts/batch_generate.py \
    --checkpoint checkpoints/stage2_best \
    --prompts-file prompts.txt \
    --output-dir outputs/generated \
    --batch-size 4
```

### æ¡ä»¶ç”Ÿæˆ

```python
# éŸ³é¢‘å¼•å¯¼çš„è§†é¢‘ç”Ÿæˆ
output = pipeline(
    audio_path="reference_audio.wav",
    prompt="A person making the sound",
    mode="audio_to_video"
)

# è§†é¢‘å¼•å¯¼çš„éŸ³é¢‘ç”Ÿæˆ
output = pipeline(
    video_path="reference_video.mp4",
    prompt="Realistic sound for this video",
    mode="video_to_audio"
)

# éŸ³è§†é¢‘ç¼–è¾‘
output = pipeline(
    audio_path="original_audio.wav",
    video_path="original_video.mp4",
    prompt="Make it more dramatic",
    mode="edit"
)
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
audio-video-generation/
â”œâ”€â”€ configs/                          # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ model_config.yaml            # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ training_config.yaml         # è®­ç»ƒé…ç½®
â”‚   â””â”€â”€ deepspeed_config.json        # DeepSpeedé…ç½®
â”‚
â”œâ”€â”€ models/                           # æ¨¡å‹å®ç°
â”‚   â”œâ”€â”€ dit_backbone.py              # DiTæ ¸å¿ƒæ¶æ„
â”‚   â”œâ”€â”€ audio_decoder.py             # éŸ³é¢‘è§£ç å™¨
â”‚   â”œâ”€â”€ video_decoder.py             # è§†é¢‘è§£ç å™¨
â”‚   â”œâ”€â”€ encoders.py                  # å„ç±»ç¼–ç å™¨
â”‚   â””â”€â”€ temporal_alignment.py        # æ—¶åºå¯¹é½æ¨¡å—
â”‚
â”œâ”€â”€ pipelines/                        # æ¨ç†Pipeline
â”‚   â”œâ”€â”€ av_generation_pipeline.py    # ç”ŸæˆPipeline
â”‚   â””â”€â”€ training_pipeline.py         # è®­ç»ƒPipeline
â”‚
â”œâ”€â”€ training/                         # è®­ç»ƒç›¸å…³
â”‚   â”œâ”€â”€ train.py                     # è®­ç»ƒä¸»è„šæœ¬
â”‚   â”œâ”€â”€ losses.py                    # æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ schedulers.py                # å­¦ä¹ ç‡è°ƒåº¦
â”‚   â””â”€â”€ callbacks.py                 # è®­ç»ƒå›è°ƒ
â”‚
â”œâ”€â”€ data/                             # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ av_dataset.py                # æ•°æ®é›†å®šä¹‰
â”‚   â”œâ”€â”€ preprocessing.py             # é¢„å¤„ç†
â”‚   â””â”€â”€ augmentation.py              # æ•°æ®å¢å¼º
â”‚
â”œâ”€â”€ evaluation/                       # è¯„ä¼°å·¥å…·
â”‚   â”œâ”€â”€ evaluate.py                  # è¯„ä¼°ä¸»è„šæœ¬
â”‚   â”œâ”€â”€ metrics.py                   # è¯„ä¼°æŒ‡æ ‡
â”‚   â””â”€â”€ human_eval.py                # äººå·¥è¯„ä¼°å·¥å…·
â”‚
â”œâ”€â”€ scripts/                          # å®ç”¨è„šæœ¬
â”‚   â”œâ”€â”€ data_crawler.py              # æ•°æ®çˆ¬å–
â”‚   â”œâ”€â”€ data_preprocessing.py        # æ•°æ®é¢„å¤„ç†
â”‚   â”œâ”€â”€ train_multi_gpu.sh           # è®­ç»ƒå¯åŠ¨è„šæœ¬
â”‚   â”œâ”€â”€ generate.py                  # ç”Ÿæˆè„šæœ¬
â”‚   â”œâ”€â”€ export_model.py              # æ¨¡å‹å¯¼å‡º
â”‚   â””â”€â”€ download_pretrained.py       # ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
â”‚
â”œâ”€â”€ utils/                            # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ audio_utils.py               # éŸ³é¢‘å·¥å…·
â”‚   â”œâ”€â”€ video_utils.py               # è§†é¢‘å·¥å…·
â”‚   â”œâ”€â”€ logging_utils.py             # æ—¥å¿—å·¥å…·
â”‚   â””â”€â”€ visualization.py             # å¯è§†åŒ–å·¥å…·
â”‚
â”œâ”€â”€ tests/                            # æµ‹è¯•
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_pipeline.py
â”‚
â”œâ”€â”€ Audio_Video_Joint_Generation_Proposal.md  # è¯¦ç»†æŠ€æœ¯æ–¹æ¡ˆ
â”œâ”€â”€ AUDIO_VIDEO_GENERATION_README.md          # æœ¬æ–‡æ¡£
â”œâ”€â”€ requirements.txt                           # Pythonä¾èµ–
â”œâ”€â”€ crawler_config.json                        # çˆ¬è™«é…ç½®
â””â”€â”€ README.md                                  # é¡¹ç›®ä¸»README
```

---

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### æ¨¡å‹æ¶æ„

```
è¾“å…¥æ¡ä»¶ (Text/Audio/Video)
    â†“
[ç¼–ç å™¨å±‚]
â”œâ”€â”€ Text: CLIP-ViT-L (1024d)
â”œâ”€â”€ Audio: Wav2Vec2 (1024d)
â””â”€â”€ Video: VideoMAE (1024d)
    â†“
[è·¨æ¨¡æ€èåˆ]
    â†“
[DiT Backbone] (2048d Ã— 24 layers)
â”œâ”€â”€ Patch Embedding
â”œâ”€â”€ Position Embedding
â”œâ”€â”€ Timestep Embedding
â”œâ”€â”€ DiT Blocks (Self-Attention + MLP)
â””â”€â”€ AdaLN (Adaptive Layer Norm)
    â†“
[æ—¶åºå¯¹é½æ¨¡å—]
â”œâ”€â”€ Audio Temporal Conv
â”œâ”€â”€ Video Temporal Conv
â””â”€â”€ Cross-Modal Attention
    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â†“               â†“
[éŸ³é¢‘è§£ç å™¨]    [è§†é¢‘è§£ç å™¨]
8å±‚Trans-        6å±‚3D-CNN
former           Decoder
    â†“               â†“
Mel-Spec        Video Latent
80-bins         4Ã—64Ã—64
    â†“               â†“
[Vocoder]       [VAE Decoder]
HiFi-GAN        SD-VAE
    â†“               â†“
Waveform        Video Frames
16kHz           512Ã—512@8fps
```

### æŸå¤±å‡½æ•°

æ€»æŸå¤±ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆ:

```python
L_total = Î»_diff * L_diffusion          # ä¸»æ‰©æ•£æŸå¤±
        + Î»_align * L_alignment         # éŸ³è§†é¢‘å¯¹é½æŸå¤±
        + Î»_perceptual * L_perceptual   # æ„ŸçŸ¥è´¨é‡æŸå¤±
        + Î»_temporal * L_temporal       # æ—¶åºä¸€è‡´æ€§æŸå¤±
```

**æƒé‡è®¾ç½®**:
- Stage 1: `Î»_diff=1.0`, å…¶ä»–ä¸º0
- Stage 2: `Î»_diff=1.0, Î»_align=0.1, Î»_perceptual=0.05`
- Stage 3: `Î»_diff=1.0, Î»_align=0.5, Î»_perceptual=0.1, Î»_temporal=0.02`

### å…³é”®æŠ€æœ¯

1. **AdaLN (Adaptive Layer Norm)**: é€šè¿‡timestepè°ƒåˆ¶normalizationå‚æ•°
2. **Flash Attention**: é«˜æ•ˆçš„attentionè®¡ç®— (2-4xåŠ é€Ÿ)
3. **Gradient Checkpointing**: ä»¥è®¡ç®—æ¢å†…å­˜ (æ”¯æŒæ›´å¤§batch size)
4. **Mixed Precision (BF16)**: åŠ é€Ÿè®­ç»ƒï¼Œå‡å°‘å†…å­˜
5. **DeepSpeed ZeRO-2**: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦

---

## ğŸ’° èµ„æºéœ€æ±‚è¯„ä¼°

### ç®—åŠ›éœ€æ±‚

| é…ç½® | GPUå‹å· | æ•°é‡ | è®­ç»ƒæ—¶é—´ | æˆæœ¬ä¼°ç®— |
|------|---------|------|----------|----------|
| **æ¨è** | A100-80GB | 128 | 8-12å‘¨ | $250K-350K |
| ç»æµ | A100-40GB | 256 | 16-24å‘¨ | $150K-250K |
| æœ€å° | A6000-48GB | 512 | 24-32å‘¨ | $100K-200K |

### å­˜å‚¨éœ€æ±‚

- **åŸå§‹æ•°æ®**: 800TB (50M samples)
- **å¤„ç†åæ•°æ®**: 500TB (WebDatasetæ ¼å¼)
- **æ£€æŸ¥ç‚¹**: 50TB (æ¯5Kæ­¥ä¿å­˜ä¸€æ¬¡)
- **ä¸´æ—¶ç¼“å­˜**: 100TB
- **æ€»è®¡**: ~1.5PB

### ç½‘ç»œå¸¦å®½

- æ•°æ®çˆ¬å–: 100Gbps Ã— 2ä¸ªæœˆ
- è®­ç»ƒé›†ç¾¤: InfiniBand 200Gbps
- æ•°æ®ä¼ è¾“åˆ°è®­ç»ƒé›†ç¾¤: 1PB @ 10Gbps = 10å¤©

---

## ğŸ“ˆ é¢„æœŸæ€§èƒ½æŒ‡æ ‡

### å®šé‡æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | SOTAå¯¹æ¯” |
|------|--------|----------|
| FAD (Audio) | < 2.0 | 2.5 (AudioLDM) |
| FVD (Video) | < 500 | 550 (CogVideo) |
| CLAP Score | > 0.30 | 0.28 |
| CLIP Score | > 0.28 | 0.26 |
| AVA Score | > 0.70 | 0.65 |
| Temporal Consistency | LPIPS < 0.15 | 0.18 |

### ç”Ÿæˆé€Ÿåº¦

- **è®­ç»ƒ**: 50 samples/sec (128 A100s)
- **æ¨ç†** (A100):
  - 10ç§’éŸ³è§†é¢‘: ~8ç§’ (50 steps)
  - æ‰¹é‡ç”Ÿæˆ (batch=8): ~15ç§’
- **ä¼˜åŒ–å** (TensorRT + INT8):
  - å•æ ·æœ¬: ~3ç§’
  - æ‰¹é‡: ~6ç§’

---

## â“ å¸¸è§é—®é¢˜

### Q1: æœ€å°GPUè¦æ±‚æ˜¯ä»€ä¹ˆ?

**A**: æ¨ç†è‡³å°‘éœ€è¦16GBæ˜¾å­˜ (å¦‚RTX 4090, A100-40GB)ã€‚è®­ç»ƒè‡³å°‘éœ€è¦8Ã—A100-40GBã€‚

### Q2: èƒ½å¦ä½¿ç”¨å¼€æºæ•°æ®é›†è€Œä¸çˆ¬å–?

**A**: å¯ä»¥ã€‚æ¨èæ•°æ®é›†:
- VGGSound (200K)
- AudioSet (2M)
- Kinetics-700 (650K)
- æ€»è®¡çº¦3Mæ ·æœ¬

ä½†è§„æ¨¡è¾ƒå°ï¼Œå»ºè®®ç»“åˆçˆ¬å–è¾¾åˆ°20M+ã€‚

### Q3: è®­ç»ƒåˆ°Stage 2éœ€è¦å¤šä¹…?

**A**: 
- 128 A100-80GB: çº¦6-8å‘¨
- 64 A100-80GB: çº¦10-12å‘¨
- 32 A100-80GB: çº¦16-20å‘¨

### Q4: å¦‚ä½•å‡å°‘è®­ç»ƒæˆæœ¬?

**A**: 
1. ä½¿ç”¨Spot/Preemptibleå®ä¾‹ (çœ50-70%)
2. å…ˆç”¨å°æ¨¡å‹éªŒè¯ (1Bå‚æ•°)
3. å‡å°‘æ•°æ®é‡åˆ°10M (çœ50%æ—¶é—´)
4. ä½¿ç”¨DeepSpeed ZeRO-3 + Offload

### Q5: æ¨¡å‹å¯ä»¥å•†ç”¨å—?

**A**: å–å†³äº:
- è®­ç»ƒæ•°æ®çš„ç‰ˆæƒ (YouTubeæœ‰é™åˆ¶)
- é¢„è®­ç»ƒæ¨¡å‹çš„License (CLIP, Wav2Vec2ç­‰)
- å»ºè®®ä½¿ç”¨å¼€æºæ•°æ®é›† + æˆæƒå†…å®¹

### Q6: å¦‚ä½•æå‡ç”Ÿæˆè´¨é‡?

**A**:
1. å¢åŠ è®­ç»ƒæ•°æ®é‡å’Œè´¨é‡
2. å»¶é•¿è®­ç»ƒæ—¶é—´
3. ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ (5B-7B)
4. åŠ å¼ºå¯¹é½æŸå¤±æƒé‡
5. ä½¿ç”¨æ›´å¥½çš„vocoder (å¦‚BigVGAN)

### Q7: æ”¯æŒå®æ—¶ç”Ÿæˆå—?

**A**: å½“å‰ä¸æ”¯æŒã€‚å®æ—¶ç”Ÿæˆéœ€è¦:
- è½»é‡çº§æ¨¡å‹ (< 500M)
- è’¸é¦æŠ€æœ¯
- ç‰¹æ®Šçš„æµå¼ç”Ÿæˆæ¶æ„
- å¯ä½œä¸ºåç»­ç ”ç©¶æ–¹å‘

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿ç¤¾åŒºè´¡çŒ®!

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºfeatureåˆ†æ”¯: `git checkout -b feature/your-feature`
3. æäº¤æ›´æ”¹: `git commit -am 'Add some feature'`
4. æ¨é€åˆ°åˆ†æ”¯: `git push origin feature/your-feature`
5. æäº¤Pull Request

ä»£ç è§„èŒƒ:
- éµå¾ªPEP 8
- æ·»åŠ å•å…ƒæµ‹è¯•
- æ›´æ–°æ–‡æ¡£
- é€šè¿‡CIæ£€æŸ¥

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ“§ è”ç³»æ–¹å¼

- é‚®ç®±: research@example.com
- GitHub Issues: https://github.com/your-org/audio-video-generation/issues
- è®ºå›: https://discuss.example.com

---

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹ä¼˜ç§€å·¥ä½œ:
- [Stable Diffusion](https://github.com/Stability-AI/stablediffusion)
- [DiT](https://github.com/facebookresearch/DiT)
- [Open-Sora](https://github.com/hpcaitech/Open-Sora)
- [AudioLDM](https://github.com/haoheliu/AudioLDM)
- [Hugging Face Diffusers](https://github.com/huggingface/diffusers)

ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰å¼€æºç¤¾åŒºçš„è´¡çŒ®è€…!

---

## ğŸ“š ç›¸å…³è®ºæ–‡

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨:

```bibtex
@article{avgen2025,
  title={Unified Diffusion Transformers for Joint Audio-Video Generation},
  author={Your Name and Collaborators},
  journal={arXiv preprint arXiv:2025.xxxxx},
  year={2025}
}
```

---

**æ›´æ–°æ—¥æœŸ**: 2025-10-28  
**ç‰ˆæœ¬**: v1.0  
**ç»´æŠ¤è€…**: Research Team

