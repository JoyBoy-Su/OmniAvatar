#!/usr/bin/env python3

import os
import sys
import io
import math
import json
import base64
import tempfile
import traceback
import librosa
import copy
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import asyncio
import threading
import queue
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from flask import Flask, request
from flask_cors import CORS
from flask_socketio import SocketIO, emit, join_room, leave_room
from PIL import Image
import time
import numpy as np
import torchvision.transforms as transforms
from collections import deque
import subprocess
import cv2
from typing import List, Optional, Dict, Any
from collections import defaultdict
import soundfile as sf
from openai import OpenAI

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from peft import LoraConfig, inject_adapter_in_model, PeftModel

from OmniAvatar.utils.args_config import parse_args
from scripts.inference import match_size, resize_pad
from OmniAvatar.utils.io_utils import save_video_as_grid_and_mp4, load_state_dict
from OmniAvatar.distributed.fsdp import shard_model
import torch.distributed as dist
import torchvision.transforms as transforms
import torch.nn.functional as F
import torchvision.transforms as TT
from transformers import Wav2Vec2FeatureExtractor
from PIL import Image
from OmniAvatar.models.model_manager import ModelManager
from OmniAvatar.prompters import WanPrompter
from OmniAvatar.schedulers.flow_match import FlowMatchScheduler
from scripts.causal_inference import CausalInferencePipeline


class QwenOmniTalker:
    """Qwen-Omniè¯­éŸ³å¯¹è¯å¤„ç†å™¨ - æ”¯æŒå¤šè½®å¯¹è¯"""
    
    def __init__(self, api_key="sk-63ad221681734d339b8171797204f105", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"):
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.system_message = {
            "role": "system",
            "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
        }
        # å­˜å‚¨æ¯ä¸ªä¼šè¯çš„å¯¹è¯å†å² {session_id: [messages]}
        self.conversation_history = {}
        # é»˜è®¤æœ€å¤§å†å²è½®æ•°ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        self.max_history_turns = 10
    
    def process_audio_conversation(self, audio_path, session_id=None, prompt="Analyze this audio and respond naturally."):
        """
        å¤„ç†éŸ³é¢‘å¯¹è¯ï¼Œè¿”å›å›å¤çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰

        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            session_id: ä¼šè¯IDï¼Œç”¨äºç”Ÿæˆå”¯ä¸€çš„è¾“å‡ºæ–‡ä»¶åå’Œç®¡ç†å¯¹è¯å†å²
            prompt: æ–‡æœ¬æç¤ºè¯ï¼Œé»˜è®¤ä¸ºåˆ†æéŸ³é¢‘å†…å®¹

        Returns:
            tuple: (reply_audio_path, reply_text) å›å¤éŸ³é¢‘è·¯å¾„å’Œæ–‡æœ¬å†…å®¹
        """
        try:
            # ä½¿ç”¨é»˜è®¤session_idå¦‚æœæœªæä¾›
            if session_id is None:
                session_id = "default"
            
            # åˆå§‹åŒ–è¯¥ä¼šè¯çš„å†å²è®°å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if session_id not in self.conversation_history:
                self.conversation_history[session_id] = []
            
            # è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶ç¼–ç ä¸ºbase64
            with open(audio_path, 'rb') as audio_file:
                audio_bytes = audio_file.read()
                audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')

            # æ„å»ºå½“å‰ç”¨æˆ·æ¶ˆæ¯
            current_user_message = {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "input_audio", "input_audio": {"data": f"data:;base64,{audio_base64}", "format": "wav"}},
                ],
            }
            
            # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼šç³»ç»Ÿæ¶ˆæ¯ + å†å²æ¶ˆæ¯ + å½“å‰æ¶ˆæ¯
            messages = [self.system_message] + self.conversation_history[session_id] + [current_user_message]
            
            # è°ƒç”¨Qwen-Omni API
            completion = self.client.chat.completions.create(
                model="qwen3-omni-flash",
                messages=messages,
                modalities=["text", "audio"],
                audio={
                    "voice": "Cherry",  # Cherry, Ethan, Serena, Chelsie is available
                    "format": "wav"
                },
                stream=True,
                stream_options={"include_usage": True}
            )
            
            # æ”¶é›†å“åº”
            text_parts = []
            audio_string = ""
            
            for chunk in completion:
                if chunk.choices:
                    if hasattr(chunk.choices[0].delta, "audio") and chunk.choices[0].delta.audio:
                        try:
                            if "data" in chunk.choices[0].delta.audio:
                                audio_string += chunk.choices[0].delta.audio["data"]
                            elif "transcript" in chunk.choices[0].delta.audio:
                                text_parts.append(chunk.choices[0].delta.audio["transcript"])
                        except Exception as e:
                            print(f"Error processing audio chunk: {e}")
                    elif hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content:
                        text_parts.append(chunk.choices[0].delta.content)
                else:
                    if hasattr(chunk, 'usage') and chunk.usage:
                        print(f"Usage: {chunk.usage}")
            
            reply_text = "".join(text_parts)
            print(f"Qwen-Omni reply text: {reply_text}")
            
            # ä¿å­˜å¯¹è¯å†å²ï¼šå°†ç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹å›å¤æ·»åŠ åˆ°å†å²è®°å½•
            # ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«æ–‡æœ¬å’ŒéŸ³é¢‘æ•°æ®ï¼‰
            user_history_message = {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "input_audio", "input_audio": {"data": f"data:;base64,{audio_base64}", "format": "wav"}},
                ]
            }
            
            # åŠ©æ‰‹å›å¤æ¶ˆæ¯
            assistant_history_message = {
                "role": "assistant",
                "content": reply_text
            }
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.conversation_history[session_id].append(user_history_message)
            self.conversation_history[session_id].append(assistant_history_message)
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘çš„ max_history_turns è½®å¯¹è¯ï¼‰
            # æ¯è½®å¯¹è¯åŒ…å«2æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·+åŠ©æ‰‹ï¼‰ï¼Œæ‰€ä»¥æ€»å…±ä¿ç•™ max_history_turns * 2 æ¡æ¶ˆæ¯
            max_messages = self.max_history_turns * 2
            if len(self.conversation_history[session_id]) > max_messages:
                self.conversation_history[session_id] = self.conversation_history[session_id][-max_messages:]
            
            print(f"Session {session_id} history: {len(self.conversation_history[session_id])} messages")
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            if audio_string:
                wav_bytes = base64.b64decode(audio_string)
                wav_array = np.frombuffer(wav_bytes, dtype=np.int16)
                
                # ç”Ÿæˆå”¯ä¸€çš„è¾“å‡ºæ–‡ä»¶å
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                session_suffix = f"_{session_id}" if session_id else ""
                reply_audio_path = f"demo_out/qwen_omni_reply_{timestamp}{session_suffix}.wav"
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(reply_audio_path), exist_ok=True)
                
                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                sf.write(reply_audio_path, wav_array, samplerate=24000)
                print(f"Qwen-Omni reply audio saved to: {reply_audio_path}")
                
                return reply_audio_path, reply_text
            else:
                print("Warning: No audio data received from Qwen-Omni")
                return None, reply_text
                
        except Exception as e:
            print(f"Error in Qwen-Omni conversation: {e}")
            traceback.print_exc()
            return None, None
    
    def clear_session_history(self, session_id=None):
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å¯¹è¯å†å²"""
        if session_id is None:
            self.conversation_history.clear()
            print("All conversation history cleared")
        elif session_id in self.conversation_history:
            del self.conversation_history[session_id]
            print(f"Session {session_id} history cleared")
        else:
            print(f"Session {session_id} not found")


class PipelinedEvalPipeline(nn.Module):
    """
    æµæ°´çº¿åŒ–çš„CausalInferencePipelineå®ç°
    ä¸»è¦æ”¹è¿›ï¼š
    1. prompt encodingæå‰åˆ°timeså¾ªç¯å¤–
    2. causal denoising inferenceå’ŒVAE decodingå®ç°æµæ°´çº¿å¹¶è¡Œ
    3. æ”¯æŒKV cacheçš„å› æœæ¨ç†
    """
    
    def __init__(self, args):
        super().__init__()
        self.args = args
        print(f"self.args: {self.args}")
        # set device and dtype
        self.device = torch.device(f"cuda:{args.rank}")
        if args.dtype=='bf16':
            self.dtype = torch.bfloat16
        elif args.dtype=='fp16':
            self.dtype = torch.float16
        else:   
            self.dtype = torch.float32
        
        # load causal inference pipeline
        self.causal_pipe = self.load_causal_model()
        
        # æµæ°´çº¿ç›¸å…³å‚æ•°
        self.denoising_queue = queue.Queue()  # denoisingä»»åŠ¡é˜Ÿåˆ—
        self.vae_queue = queue.Queue()  # decodingä»»åŠ¡é˜Ÿåˆ—
        self.result_buffer = {}  # ä¿æŒå­—å…¸ç»“æ„ï¼Œç”¨äºæŒ‰chunk_idæ’åº
        self.latents_buffer = {}
        # å¤šçº¿ç¨‹æ§åˆ¶
        self.denoising_thread = None
        self.vae_thread = None
        self.result_lock = threading.Lock()
        self.latents_lock = threading.Lock()
        self.stop_workers = threading.Event()  # Signal to stop worker threads
        # äº‹ä»¶è§¦å‘åŒæ­¥æœºåˆ¶
        self.denoising_events = deque()
        self.vae_events = deque()
        self.current_clock = 0
        
        # æ€§èƒ½ç»Ÿè®¡ç›¸å…³
        self.timing_stats = defaultdict(list)  # å­˜å‚¨å„æ­¥éª¤çš„è€—æ—¶ç»Ÿè®¡
        self.timing_lock = threading.Lock()  # ä¿æŠ¤timing_statsçš„çº¿ç¨‹é”
    
    def load_causal_model(self):
        """Load causal inference pipeline"""
        # Initialize model manager
        model_manager = ModelManager(device="cpu", infer=True)
        model_manager.load_models(
            [
                self.args.dit_path.split(","),   # load dit
                self.args.text_encoder_path,     # load text encoder
                self.args.vae_path               # load vae
            ],
            torch_dtype=self.dtype,
            device='cpu',
        )
        
        # Create causal inference pipeline
        causal_pipe = CausalInferencePipeline.from_model_manager(
            model_manager=model_manager,
            args=self.args,
            device=self.device
        )
        
        # Move VAE to different GPU for pipeline parallelism
        causal_pipe.vae.to("cuda:1")
        print(f"Move VAE to cuda:1 for pipeline parallelism")
        
        return causal_pipe
    
    def run_pipeline(
            self,
            noise: torch.Tensor,
            batch_size: int,
            num_blocks: int,
            num_input_frames: int,
            initial_latent: torch.Tensor,
            conditional_dict: dict,
            img_lat: torch.Tensor,
            output: torch.Tensor,
            audio_path: str,
            id: str = None
        ):
        """
        Run pipelined causal inference
        
        Args:
            noise_blocks: List of noise tensors for each block
            text_prompts: Text prompts for generation
            image_path: Path to reference image
            audio_path: Path to audio file
            initial_latent: Initial latent for I2V
            id: Unique identifier for this generation session
            return_latents: Whether to return latents
        """
        # Stop old worker threads if they exist
        if self.denoising_thread is not None or self.vae_thread is not None:
            print("Stopping old worker threads...")
            self.stop_workers.set()
            if self.denoising_thread is not None and self.denoising_thread.is_alive():
                self.denoising_thread.join(timeout=2.0)
            if self.vae_thread is not None and self.vae_thread.is_alive():
                self.vae_thread.join(timeout=2.0)
            self.stop_workers.clear()
            print("Old worker threads stopped")

        # clear pipeline state
        self.current_clock = 0
        self.denoising_queue.queue.clear()
        self.vae_queue.queue.clear()
        self.result_buffer.clear()
        self.denoising_events.clear()
        self.vae_events.clear()
        
        # æ¸…ç©ºæ€§èƒ½ç»Ÿè®¡
        with self.timing_lock:
            self.timing_stats.clear()
        
        # run async threads
        self.denoising_thread = threading.Thread(target=self._causal_denoising_worker, daemon=True)
        self.vae_thread = threading.Thread(target=self._vae_worker, daemon=True)
        self.denoising_thread.start()
        self.vae_thread.start()
        
        # Step 2: cache context feature
        current_start_frame_global = 0
        current_start_frame_local = 0
        if initial_latent is not None:
            print("INITIAL_LATENT is not None!!")
            # raise ValueError
        
        # Step 3: temporal denoising loop
        print(f"NUM BLOCKS is {num_blocks}")
        all_num_frames = [self.args.num_frame_per_block] * num_blocks
        for current_num_frames in all_num_frames:
            print(f"===================== Current clock: {self.current_clock} ======================")
            print(f"Processing frame {current_start_frame_global - num_input_frames} to {current_start_frame_global + current_num_frames - num_input_frames}.")
            noisy_input = noise[:, current_start_frame_global - num_input_frames:current_start_frame_global + current_num_frames - num_input_frames]
            y_input = conditional_dict["image"][:, :, current_start_frame_local - num_input_frames:current_start_frame_local + current_num_frames - num_input_frames]
            audio_input = conditional_dict["audio"][:,current_start_frame_global - num_input_frames:current_start_frame_global + current_num_frames - num_input_frames]
            block_conditional_dict = conditional_dict.copy()
            block_conditional_dict.update(image=y_input.clone(), audio=audio_input.clone())
            
            denoising_task = {
                "chunk_id": self.current_clock,
                "current_start_frame": current_start_frame_local,
                "current_num_frames": current_num_frames,
                "img_lat": img_lat.clone(),
                "batch_size": batch_size,
                "noisy_input": noisy_input.clone(),
                "block_conditional_dict": block_conditional_dict,
                "output": output.clone()
            }
            self.denoising_queue.put(denoising_task)
            # wait denoising clock and decoding clock - 1
            self.denoising_events.append(threading.Event())
            self.denoising_events[self.current_clock].wait()
            if self.current_clock >= 1:
                self.vae_events[self.current_clock - 1].wait()
            # Step 3.4: update the start and end frame indices
            current_start_frame_local += current_num_frames
            current_start_frame_global += current_num_frames
            # Step 3.5: Update img_lat every 3 blocks with latest generated latents
            # After denoising is complete, we can safely access output
            # Update after completing blocks 2, 5, 8, ... (i.e., when self.current_clock is 2, 5, 8, ...)
            # This ensures blocks 3, 6, 9, ... will use the updated img_lat
            # if reset, wait vae_events[self.current_clock]
            if (self.current_clock + 1) % 3 == 0 and self.current_clock >= 2:
                print(f"Updating img_lat after block {self.current_clock} with latest generated latents")
                # Extract the latest generated latents from output
                self.vae_events[self.current_clock].wait()
                # get last frame
                with self.result_lock:
                    last_block_frames = self.result_buffer[self.current_clock]["video"]
                    print(f"last_block_frames shape: {last_block_frames.shape}")
                last_frame = last_block_frames[:, -1:]  # (batch 1, frame 1, chanel 3, h 400, w 720)
                # save last frame as image
                for frame_idx in range(last_block_frames.shape[1]):
                    frame = last_block_frames[:, frame_idx]
                    frame_np = (frame.squeeze(0).squeeze(0).permute(1, 2, 0).cpu().float().numpy() * 255).astype('uint8')
                    frame_pil = Image.fromarray(frame_np)
                    frame_pil.save(f"examples/chunk_id_{self.current_clock}_frame_{frame_idx}.png")
                import pdb; pdb.set_trace()
                # target shape (batch 1, channel 3, frame 1, h 400, w 720)
                last_frame = last_frame.permute(0, 2, 1, 3, 4)
                print(f"last_frame shape: {last_frame.shape}")
                
                # encode last frame to img_lat (following the logic in forward function)
                encode_start_time = time.time()
                
                # Ensure last_frame is in the correct range [-1, 1] for VAE encoding
                # last_frame should already be in [0, 1] or [-1, 1] from VAE decode
                # If it's in [0, 1], convert to [-1, 1]
                if last_frame.min() >= 0:
                    print(f"last_frame.min() >= 0, convert to [-1, 1]")
                    last_frame = last_frame * 2.0 - 1.0
                
                # VAE encode to get latent representation
                self.causal_pipe.vae.to("cuda:1")
                new_img_lat = self.causal_pipe.vae.encode(
                    videos=last_frame.to(dtype=self.dtype),
                    device="cuda:1"
                )  # Shape: (batch, 16, 1, h, w)
                print(f"Encoded new_img_lat shape: {new_img_lat.shape}")
                
                # Repeat to match num_frames dimension (same as original img_lat)
                num_frames_for_img_lat = img_lat.shape[2]  # Use original img_lat's num_frames
                new_img_lat = new_img_lat.repeat(1, 1, num_frames_for_img_lat, 1, 1)  # (batch, 16, num_frames, h, w)
                print(f"Repeated new_img_lat shape: {new_img_lat.shape}")
                
                # Create mask: first frame mask=0 (fixed), rest mask=1 (can be generated)
                msk = torch.zeros_like(new_img_lat)[:, :1]  # (batch, 1, num_frames, h, w)
                msk[:, :, 1:] = 1
                
                # Concatenate latents and mask to get final img_lat format
                img_lat = torch.cat([new_img_lat, msk], dim=1)  # (batch, 17, num_frames, h, w)
                print(f"Updated img_lat shape: {img_lat.shape}")
                
                # Update conditional_dict with new img_lat for all future frames
                total_num_frames = noise.shape[1]
                # if img_lat.shape[2] < total_num_frames:
                #     # Repeat img_lat to match total num_frames
                #     print(f"img_lat.shape[2] < total_num_frames, img_lat.shape[2]: {img_lat.shape[2]}, total_num_frames: {total_num_frames}")
                    # repeat_factor = total_num_frames // img_lat.shape[2] + 1
                    # img_lat_repeated = img_lat.repeat(1, 1, repeat_factor, 1, 1)
                    # img_lat_repeated = img_lat_repeated[:, :, :total_num_frames]
                # else:
                #     print(f"img_lat.shape[2] >= total_num_frames, img_lat.shape[2]: {img_lat.shape[2]}, total_num_frames: {total_num_frames}")  
                #     img_lat_repeated = img_lat[:, :, :total_num_frames]
                
                # img_lat = img_lat_repeated.clone()
                conditional_dict["image"] = img_lat.clone()
                print(f"img_lat shape: {img_lat.shape}")
                print(f"updated conditional_dict['image'] shape: {conditional_dict['image'].shape}")
                import pdb; pdb.set_trace()
                encode_time = time.time() - encode_start_time
                self._record_timing("img_lat_update_encode", encode_time, self.current_clock)
                # import pdb; pdb.set_trace()
                # reset
                self.causal_pipe.vae.clear_cache()
                self.causal_pipe.reset_caches(noise.device)
                current_start_frame_local = 0
                
            self.current_clock += 1
        
        # Wait for all decoding to complete
        self.vae_events[self.current_clock - 1].wait()
        
        # æ‰“å°æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š
        self._print_timing_report()
        # ä¿å­˜å®Œæ•´è§†é¢‘å¹¶åˆå¹¶éŸ³é¢‘
        self._save_complete_video_with_audio(num_blocks, audio_path, id)
  
    def _record_timing(self, step_name: str, duration: float, chunk_id: int = None):
        """è®°å½•æ­¥éª¤è€—æ—¶"""
        with self.timing_lock:
            key = f"{step_name}_chunk_{chunk_id}" if chunk_id is not None else step_name
            self.timing_stats[key].append(duration)
    
    def _print_timing_report(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("PERFORMANCE TIMING REPORT")
        print("="*80)
        
        with self.timing_lock:
            # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
            forward_steps = {}
            denoising_steps = {}
            vae_steps = {}
            send_steps = {}
            
            for key, times in self.timing_stats.items():
                if key.startswith('forward_'):
                    forward_steps[key] = times
                elif 'denoising' in key or 'generator_forward' in key:
                    denoising_steps[key] = times
                elif 'vae' in key or 'decode' in key:
                    vae_steps[key] = times
                elif 'send' in key:
                    send_steps[key] = times
            
            # æ‰“å°Forwardé˜¶æ®µç»Ÿè®¡
            if forward_steps:
                print("\nğŸ“Š FORWARD PHASE TIMING:")
                print("-" * 50)
                total_forward_time = 0
                for step, times in forward_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_forward_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL FORWARD TIME':<35}: {total_forward_time:>8.3f}s")
            
            # æ‰“å°Denoisingé˜¶æ®µç»Ÿè®¡
            if denoising_steps:
                print("\nğŸ”„ DENOISING PHASE TIMING:")
                print("-" * 50)
                total_denoising_time = 0
                for step, times in denoising_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_denoising_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL DENOISING TIME':<35}: {total_denoising_time:>8.3f}s")
            
            # æ‰“å°VAEé˜¶æ®µç»Ÿè®¡
            if vae_steps:
                print("\nğŸ¬ VAE DECODE PHASE TIMING:")
                print("-" * 50)
                total_vae_time = 0
                for step, times in vae_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_vae_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL VAE TIME':<35}: {total_vae_time:>8.3f}s")

            if send_steps:
                print("\nğŸ“¤ SEND PHASE TIMING:")
                print("-" * 50)
                total_send_time = 0
                for step, times in send_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_send_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL SEND TIME':<35}: {total_send_time:>8.3f}s")
            
            # æ‰“å°æ€»ä½“ç»Ÿè®¡
            print("\nğŸ“ˆ OVERALL STATISTICS:")
            print("-" * 50)
            total_pipeline_time = sum(np.sum(times) for times in self.timing_stats.values())
            print(f"  {'TOTAL PIPELINE TIME':<35}: {total_pipeline_time:>8.3f}s")
            
            # åˆ†æç“¶é¢ˆ
            print("\nğŸ” BOTTLENECK ANALYSIS:")
            print("-" * 50)
            step_totals = {step: np.sum(times) for step, times in self.timing_stats.items()}
            sorted_steps = sorted(step_totals.items(), key=lambda x: x[1], reverse=True)
            for i, (step, total_time) in enumerate(sorted_steps[:5]):
                percentage = (total_time / total_pipeline_time) * 100 if total_pipeline_time > 0 else 0
                print(f"  {i+1}. {step:<30}: {total_time:>8.3f}s ({percentage:>5.1f}%)")
        
        print("="*80)

    @torch.no_grad()
    def forward(
        self,
        noise: torch.Tensor,
        text_prompts: str,
        image_path: Optional[str] = None,
        audio_path: Optional[str] = None,
        initial_latent: Optional[torch.Tensor] = None,
        return_latents: bool = False,
        id: str = None
    ) -> torch.Tensor:
        """
        Forward pass with automatic conditioning initialization.
        
        Args:
            noise: Input noise tensor [batch_size, num_output_frames, channels, height, width]
            text_prompts: Text prompts for generation
            image_path: Path to reference image (optional)
            audio_path: Path to audio file (optional)
            initial_latent: Initial latent for I2V [batch_size, num_input_frames, channels, height, width]
            return_latents: Whether to return latents
            id: Unique identifier for this generation session
            
        Returns:
            Generated video tensor [batch_size, num_frames, channels, height, width]
        """
        # Calculate required number of frames from noise tensor
        batch_size, num_frames, num_channels, height, width = noise.shape

        # prepare image condition
        if image_path is not None:
            start_time = time.time()
            from PIL import Image
            image = Image.open(image_path).convert("RGB")
            image = self.causal_pipe.transform(image).unsqueeze(0).to(self.device)
            _, _, h, w = image.shape
            select_size = match_size(getattr(self.args, f'image_sizes_{self.args.max_hw}'), h, w)
            image = resize_pad(image, (h, w), select_size)
            print(f"image shape after resize: {image.shape}")
            
            # save original image (before normalization)
            # image shape should be (1, 3, h, w) in range [0, 1]
            image_to_save = image.squeeze(0).permute(1, 2, 0)  # (h, w, 3)
            image_np = (image_to_save.cpu().float().numpy() * 255).astype('uint8')
            image_pil = Image.fromarray(image_np)
            image_pil.save("examples/original_image.png")
            print(f"Saved original image with shape: {image_np.shape}")
            
            # Now normalize for model input
            image = image * 2.0 - 1.0  # Convert to [-1, 1]
            image = image[:, :, None]
            print(f"encode image shape: {image.shape}")
            # import pdb; pdb.set_trace()
            image_prep_time = time.time() - start_time
            self._record_timing("forward_image_preprocessing", image_prep_time)
            
            start_time = time.time()
            self.causal_pipe.vae.to("cuda:1")
            # Use num_frames from noise tensor instead of hardcoded 21
            # img_latçš„ä½œç”¨æœºåˆ¶ï¼š
            # 1. ä½œä¸ºæ¡ä»¶ä¿¡æ¯ï¼šimg_latå­˜å‚¨åœ¨conditional_dict["image"]ä¸­ï¼Œä¼ é€’ç»™generator_forwardä½œä¸ºå›¾åƒæ¡ä»¶
            # 2. ä½œä¸ºåˆå§‹å¸§å¼•å¯¼ï¼šåœ¨ç¬¬ä¸€ä¸ªblockï¼ˆcurrent_start_frame==0ï¼‰æ—¶ï¼Œimg_lat[:, :16, :1]ç”¨äºåˆå§‹åŒ–ç¬¬ä¸€å¸§
            # 3. æ ¼å¼ï¼š(batch, 17, num_frames, h, w)ï¼Œå…¶ä¸­å‰16ä¸ªé€šé“æ˜¯VAE latentï¼Œæœ€å1ä¸ªé€šé“æ˜¯mask
            # 4. maskè§„åˆ™ï¼šç¬¬ä¸€å¸§mask=0ï¼ˆå›ºå®šï¼‰ï¼Œåç»­å¸§mask=1ï¼ˆå¯ç”Ÿæˆï¼‰
            # 5. åœ¨run_pipelineä¸­ï¼Œæ¯éš”3ä¸ªblockä¼šç”¨æœ€æ–°ç”Ÿæˆçš„latentsæ›´æ–°img_latï¼Œå®ç°åŠ¨æ€å¼•å¯¼
            img_lat = self.causal_pipe.vae.encode(videos=image.to(dtype=self.dtype),device="cuda:1").repeat(1,1,num_frames,1,1)
            msk = torch.zeros_like(img_lat)[:,:1]
            msk[:, :, 1:] = 1
            img_lat = torch.cat([img_lat, msk], dim=1)
            image_encode_time = time.time() - start_time
            self._record_timing("forward_image_vae_encode", image_encode_time)
            print("img_lat:",img_lat.shape)
        
        # prepare audio_condition
        if audio_path is not None:
            start_time = time.time()
            audio, sr = librosa.load(audio_path, sr=self.args.sample_rate)
        
            input_values = np.squeeze(
                    self.causal_pipe.wav_feature_extractor(audio, sampling_rate=16000).input_values # TODO: update sample rate to
                )
            input_values = torch.from_numpy(input_values).float().to(device=self.device)
            audio_len = (noise.shape[1] - 1) * 4 + 1
            input_values = input_values.unsqueeze(0)
            audio_prep_time = time.time() - start_time
            self._record_timing("forward_audio_preprocessing", audio_prep_time)
            
            start_time = time.time()
            with torch.no_grad():
                self.causal_pipe.audio_encoder.to(self.device)
                hidden_states = self.causal_pipe.audio_encoder(input_values, seq_len=audio_len, output_hidden_states=True)
                audio_embeddings = hidden_states.last_hidden_state
                for mid_hidden_states in hidden_states.hidden_states:
                    audio_embeddings = torch.cat((audio_embeddings, mid_hidden_states), -1)
                audio_emb = audio_embeddings.permute(0, 2, 1)[:, :, :, None, None]
                audio_emb = torch.cat([audio_emb[:, :, :1].repeat(1, 1, 3, 1, 1), audio_emb], 2) # 1, 768, 44, 1, 1
                audio_emb = self.causal_pipe.generator.audio_proj(audio_emb.to(self.dtype))
                audio_emb = torch.concat([audio_cond_proj(audio_emb) for audio_cond_proj in self.causal_pipe.generator.audio_cond_projs], 0)
                print("audio_shape:",audio_emb.shape)
            audio_encode_time = time.time() - start_time
            self._record_timing("forward_audio_encoding", audio_encode_time)
        else:
            print("Detect No audio input!!")
            audio_embeddings = None
        
        # inference (prepare for run_pipeline)
        batch_size, num_frames, num_channels, height, width = noise.shape
        # frame block calculations
        assert num_frames % self.args.num_frame_per_block == 0
        num_blocks = num_frames // self.args.num_frame_per_block
        num_input_frames = initial_latent.shape[1] if initial_latent is not None else 0
        num_output_frames = num_frames + num_input_frames
        # text conditioning
        start_time = time.time()
        self.causal_pipe.text_encoder.to("cuda")
        self.causal_pipe.vae.clear_cache()
        conditional_dict = self.causal_pipe.encode_text_prompts(text_prompts, positive=True)
        conditional_dict["image"] = img_lat
        conditional_dict["audio"] = audio_emb
        text_encode_time = time.time() - start_time
        self._record_timing("forward_text_encoding", text_encode_time)
        
        output = torch.zeros(
            [batch_size, num_output_frames, num_channels, height, width],
            device=noise.device,
            dtype=noise.dtype
        )
        # step 1: initialize KV caches
        start_time = time.time()
        self.causal_pipe.setup_caches(batch_size, noise.dtype, noise.device)
        cache_setup_time = time.time() - start_time
        self._record_timing("forward_cache_setup", cache_setup_time)
        
        # run pipeline
        self.run_pipeline(
            noise=noise,
            batch_size=batch_size,
            num_blocks=num_blocks,
            num_input_frames=num_input_frames,
            initial_latent=initial_latent,
            conditional_dict=conditional_dict,
            img_lat=img_lat,
            output=output,
            audio_path=audio_path,
            id=id
        )
    
    @torch.no_grad()
    def _causal_denoising_worker(self):
        """
        Causal denoising worker
        ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡å¹¶æ‰§è¡Œå› æœæ¨ç†
        """
        while True:
            try:
                task = self.denoising_queue.get(timeout=0.1)
                chunk_id = task["chunk_id"]
                print(f"Processing causal denoising inference for chunk {chunk_id}")
                
                # è®°å½•æ•´ä¸ªdenoisingä»»åŠ¡çš„å¼€å§‹æ—¶é—´
                denoising_start_time = time.time()
                
                current_start_frame = task["current_start_frame"]
                current_num_frames = task["current_num_frames"]
                img_lat = task["img_lat"]
                batch_size = task["batch_size"]
                noisy_input = task["noisy_input"]
                block_conditional_dict = task["block_conditional_dict"]
                output = task["output"]
                # import pdb; pdb.set_trace()
                # Step 3.1: Spatial denoising loop
                # import pdb; pdb.set_trace()
                denoising_loop_start = time.time()
                for index, current_timestep in enumerate(self.causal_pipe.denoising_step_list):
                    step_start_time = time.time()
                    
                    if current_start_frame == 0:
                        noisy_input[:, :1] = img_lat[:, :16, :1].permute(0, 2, 1, 3, 4)
                    timestep = torch.ones([batch_size, current_num_frames], device=noisy_input.device, dtype=torch.int64) * current_timestep
                    
                    # generate
                    generator_start_time = time.time()
                    v, denoised_pred = self.causal_pipe.generator_forward(
                        noisy_image_or_video=noisy_input,
                        conditional_dict=block_conditional_dict,
                        timestep=timestep,
                        kv_cache=self.causal_pipe.kv_cache1,
                        crossattn_cache=self.causal_pipe.crossattn_cache,
                        current_start=current_start_frame * self.causal_pipe.frame_seq_length
                    )
                    generator_time = time.time() - generator_start_time
                    self._record_timing(f"denoising_generator_forward_step_{index}", generator_time, chunk_id)
                    
                    if index < len(self.causal_pipe.denoising_step_list) - 1:
                        noise_start_time = time.time()
                        next_timestep = self.causal_pipe.denoising_step_list[index + 1]
                        noisy_input = self.causal_pipe.scheduler.add_noise(
                            denoised_pred.flatten(0, 1),
                            torch.randn_like(denoised_pred.flatten(0, 1)),
                            next_timestep * torch.ones([batch_size * current_num_frames], device=noisy_input.device, dtype=torch.long)
                        ).unflatten(0, denoised_pred.shape[:2])
                        noise_time = time.time() - noise_start_time
                        self._record_timing(f"denoising_add_noise_step_{index}", noise_time, chunk_id)
                    
                    step_time = time.time() - step_start_time
                    self._record_timing(f"denoising_total_step_{index}", step_time, chunk_id)
                
                denoising_loop_time = time.time() - denoising_loop_start
                self._record_timing("denoising_loop_total", denoising_loop_time, chunk_id)
                
                # Step 3.2: record the model's output
                if current_start_frame == 0:
                    denoised_pred[:, :1] = img_lat[:, :16, :1].permute(0, 2, 1, 3, 4)
                output[:, current_start_frame:current_start_frame + current_num_frames] = denoised_pred # latents: denoised_pred
                
                # Step 3.3: return with timestep zero to update KV cache using clean context
                context_start_time = time.time()
                context_timestep = torch.ones_like(timestep) * 0
                self.causal_pipe.generator_forward(
                    noisy_image_or_video=denoised_pred,
                    conditional_dict=block_conditional_dict,
                    timestep=context_timestep,
                    kv_cache=self.causal_pipe.kv_cache1,
                    crossattn_cache=self.causal_pipe.crossattn_cache,
                    current_start=current_start_frame * self.causal_pipe.frame_seq_length,
                )
                context_time = time.time() - context_start_time
                self._record_timing("denoising_context_update", context_time, chunk_id)
                
                # è®°å½•æ•´ä¸ªdenoisingä»»åŠ¡çš„æ€»æ—¶é—´
                total_denoising_time = time.time() - denoising_start_time
                self._record_timing("denoising_total", total_denoising_time, chunk_id)
                
                # è®°å½•æ•´ä¸ªdenoisingä»»åŠ¡çš„æ€»æ—¶é—´
                total_denoising_time = time.time() - denoising_start_time
                self._record_timing("denoising_total", total_denoising_time, chunk_id)

                latents = output[:, current_start_frame:current_start_frame + current_num_frames]
                print(f"chunk_id: {chunk_id}, latents: {latents.shape}")
                task.update({
                    "latents": latents.clone()
                })
                self.vae_queue.put(task)
                with self.latents_lock:
                    self.latents_buffer[chunk_id] = latents.clone()
                print(f"Causal denoising inference completed for block {chunk_id}")
                # Trigger denoising event - use chunk_id instead of self.current_clock to avoid race condition
                self.denoising_events[chunk_id].set()
                
                # Create VAE event for this block
                self.vae_events.append(threading.Event())
                
                self.denoising_queue.task_done()

            except queue.Empty:
                # Check if we should stop
                if self.stop_workers.is_set():
                    print("Denoising worker received stop signal")
                    break
                pass
            except Exception as e:
                print(f"Error in causal denoising inference for block {chunk_id}: {e}")
                traceback.print_exc()
                self.denoising_queue.task_done()
    
    @torch.no_grad()
    def _vae_worker(self):
        """
        VAE processing worker (mainly for format conversion and streaming)
        """
        while True:
            try:
                task = self.vae_queue.get(timeout=0.1)
                chunk_id = task["chunk_id"]
                print(f"Processing video formatting for block {chunk_id}")
                
                # è®°å½•æ•´ä¸ªVAEä»»åŠ¡çš„å¼€å§‹æ—¶é—´
                vae_start_time = time.time()
                
                latents: torch.Tensor = task["latents"]
                
                # æ•°æ®å‡†å¤‡é˜¶æ®µ
                prep_start_time = time.time()
                latents = latents.permute(0, 2, 1, 3, 4)    # (b, c, t, h, w)
                latents = latents.to("cuda:1")
                prep_time = time.time() - prep_start_time
                self._record_timing("vae_data_preparation", prep_time, chunk_id)
                
                # VAEè§£ç é˜¶æ®µ
                decode_start_time = time.time()
                video = self.causal_pipe.vae.decode(latents, device="cuda:1", tiled=False, tile_size=(30, 52), tile_stride=(15, 26)).permute(0, 2, 1, 3, 4)
                print(f"video shape: {video.shape}")
                decode_time = time.time() - decode_start_time
                self._record_timing("vae_decode", decode_time, chunk_id)
                
                # åå¤„ç†é˜¶æ®µ
                postprocess_start_time = time.time()
                # video = video[:, :, 1:].permute(0, 2, 1, 3, 4)
                # video = video[:, :, 1:]
                # Ensure video is in float32 for compatibility with numpy conversion
                # video = video[:, 1:]
                print(f"Video before float conversion - dtype: {video.dtype}, shape: {video.shape}")
                video = (video.float() + 1) / 2  # Normalize from [-1, 1] to [0, 1]
                print(f"Video after float conversion - dtype: {video.dtype}, shape: {video.shape}")
                postprocess_time = time.time() - postprocess_start_time
                self._record_timing("vae_postprocessing", postprocess_time, chunk_id)
                
                # Store result in buffer with lock
                buffer_start_time = time.time()
                task.update({"video": video})
                with self.result_lock:
                    self.result_buffer[chunk_id] = task
                buffer_time = time.time() - buffer_start_time
                self._record_timing("vae_buffer_storage", buffer_time, chunk_id)
                
                # è®°å½•æ•´ä¸ªVAEä»»åŠ¡çš„æ€»æ—¶é—´
                total_vae_time = time.time() - vae_start_time
                self._record_timing("vae_total", total_vae_time, chunk_id)
                
                print(f"Video formatting completed for block {chunk_id}")
                # trigger vae event - use chunk_id instead of self.current_clock - 1 to avoid race condition
                self.vae_events[chunk_id].set()
                self.vae_queue.task_done()
            except queue.Empty:
                # Check if we should stop
                if self.stop_workers.is_set():
                    print("VAE worker received stop signal")
                    break
                pass
            except Exception as e:
                print(f"Error in video formatting for block {chunk_id}: {e}")
                traceback.print_exc()
                self.vae_queue.task_done()

    def _save_complete_video_with_audio(self, num_blocks: int, audio_path: Optional[str], id: Optional[str]):
        """
        ä¿å­˜å®Œæ•´è§†é¢‘å¹¶ä¸éŸ³é¢‘åˆå¹¶
        
        Args:
            num_blocks: æ€»çš„å—æ•°
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            id: å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
        """
        print("\n" + "="*80)
        print("SAVING COMPLETE VIDEO WITH AUDIO")
        print("="*80)
        
        save_start_time = time.time()
        
        # æ­¥éª¤1: ä»result_bufferæ”¶é›†æ‰€æœ‰è§†é¢‘å—
        print(f"æ”¶é›† {num_blocks} ä¸ªè§†é¢‘å—...")
        all_videos = []
        for chunk_id in range(num_blocks):
            if chunk_id not in self.result_buffer:
                print(f"è­¦å‘Š: chunk {chunk_id} ä¸åœ¨ result_buffer ä¸­")
                continue
            
            video_chunk = self.result_buffer[chunk_id]["video"]
            all_videos.append(video_chunk)
            print(f"  Chunk {chunk_id}: shape {video_chunk.shape}")
        
        if len(all_videos) == 0:
            print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘å—!")
            return
        
        # æ­¥éª¤2: æ‹¼æ¥æ‰€æœ‰è§†é¢‘å—
        print(f"æ‹¼æ¥ {len(all_videos)} ä¸ªè§†é¢‘å—...")
        complete_video = torch.cat(all_videos, dim=1)  # åœ¨æ—¶é—´ç»´åº¦æ‹¼æ¥
        print(f"å®Œæ•´è§†é¢‘ shape: {complete_video.shape}")
        
        # æ­¥éª¤3: è½¬æ¢ä¸ºnumpyæ•°ç»„
        print("å°†è§†é¢‘è½¬æ¢ä¸º numpy æ•°ç»„...")
        # Shape: (batch, time, channels, height, width) -> (time, height, width, channels)
        video_np = complete_video.squeeze(0).permute(0, 2, 3, 1).cpu().float().numpy()
        video_np = (video_np * 255).astype(np.uint8)
        print(f"è§†é¢‘ numpy shape: {video_np.shape}")
        
        # æ­¥éª¤4: åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "output_videos"
        os.makedirs(output_dir, exist_ok=True)
        
        # æ­¥éª¤5: ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        id_str = f"_{id}" if id else ""
        video_no_audio_path = os.path.join(output_dir, f"video_no_audio{id_str}_{timestamp}.mp4")
        video_with_audio_path = os.path.join(output_dir, f"video_with_audio{id_str}_{timestamp}.mp4")
        
        # æ­¥éª¤6: ä¿å­˜æ— éŸ³é¢‘è§†é¢‘
        print(f"ä¿å­˜æ— éŸ³é¢‘è§†é¢‘åˆ°: {video_no_audio_path}")
        fps = getattr(self.args, 'fps', 16)
        height, width = video_np.shape[1:3]
        
        # ä½¿ç”¨cv2ä¿å­˜è§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(video_no_audio_path, fourcc, fps, (width, height))
        
        for frame_idx in range(video_np.shape[0]):
            frame_bgr = cv2.cvtColor(video_np[frame_idx], cv2.COLOR_RGB2BGR)
            video_writer.write(frame_bgr)
        
        video_writer.release()
        video_save_time = time.time() - save_start_time
        print(f"âœ“ æ— éŸ³é¢‘è§†é¢‘ä¿å­˜æˆåŠŸ (ç”¨æ—¶ {video_save_time:.2f}s)")
        
        # æ­¥éª¤7: å¦‚æœæœ‰éŸ³é¢‘è·¯å¾„ï¼Œä½¿ç”¨ffmpegåˆå¹¶éŸ³é¢‘
        if audio_path is not None and os.path.exists(audio_path):
            print(f"\nä½¿ç”¨ ffmpeg åˆå¹¶éŸ³é¢‘: {audio_path}")
            merge_start_time = time.time()
            
            try:
                # æ„å»ºffmpegå‘½ä»¤
                ffmpeg_cmd = [
                    'ffmpeg',
                    '-i', video_no_audio_path,  # è¾“å…¥è§†é¢‘
                    '-i', audio_path,            # è¾“å…¥éŸ³é¢‘
                    '-map', '0:v',               # é€‰æ‹©ç¬¬ä¸€ä¸ªè¾“å…¥çš„è§†é¢‘æµ
                    '-map', '1:a',               # é€‰æ‹©ç¬¬äºŒä¸ªè¾“å…¥çš„éŸ³é¢‘æµ
                    '-c:v', 'libx264',           # è§†é¢‘ç¼–ç å™¨
                    '-preset', 'medium',         # ç¼–ç é¢„è®¾
                    '-crf', '23',                # è´¨é‡å‚æ•°
                    '-c:a', 'aac',               # éŸ³é¢‘ç¼–ç å™¨
                    '-b:a', '192k',              # éŸ³é¢‘æ¯”ç‰¹ç‡
                    '-shortest',                 # ä»¥æœ€çŸ­çš„æµä¸ºå‡†
                    '-y',                        # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                    video_with_audio_path
                ]
                
                print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(ffmpeg_cmd)}")
                result = subprocess.run(
                    ffmpeg_cmd,
                    capture_output=True,
                    text=True,
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                
                if result.returncode == 0:
                    merge_time = time.time() - merge_start_time
                    print(f"âœ“ éŸ³ç”»åˆå¹¶è§†é¢‘ä¿å­˜æˆåŠŸ: {video_with_audio_path}")
                    print(f"  éŸ³é¢‘åˆå¹¶ç”¨æ—¶: {merge_time:.2f}s")
                else:
                    print(f"âœ— ffmpeg é”™è¯¯ (è¿”å›ç : {result.returncode})")
                    print(f"  stderr: {result.stderr}")
                    print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
                    
            except subprocess.TimeoutExpired:
                print("âœ— ffmpeg è¶…æ—¶ (è¶…è¿‡5åˆ†é’Ÿ)")
                print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
            except FileNotFoundError:
                print("âœ— æœªæ‰¾åˆ° ffmpeg å‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£… ffmpeg")
                print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
            except Exception as e:
                print(f"âœ— éŸ³é¢‘åˆå¹¶å¼‚å¸¸: {e}")
                traceback.print_exc()
                print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
        else:
            if audio_path is None:
                print("\næœªæä¾›éŸ³é¢‘è·¯å¾„ï¼Œè·³è¿‡éŸ³é¢‘åˆå¹¶")
            else:
                print(f"\néŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}ï¼Œè·³è¿‡éŸ³é¢‘åˆå¹¶")
        
        total_save_time = time.time() - save_start_time
        print(f"\næ€»ä¿å­˜æ—¶é—´: {total_save_time:.2f}s")
        print("="*80 + "\n")
    
    def process_multiturn_conversation(
        self,
        conversation_data: Dict[str, Any],
        image_path: str,
        id: Optional[str] = None
    ):
        """
        å¤„ç†å¤šè½®å¯¹è¯ï¼Œä¸ºæ¯ä¸€è½®ç”Ÿæˆè§†é¢‘ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰
        
        Args:
            conversation_data: åŒ…å«conversationåˆ—è¡¨çš„å­—å…¸ï¼Œæ ¼å¼å¦‚ï¼š
                {
                    "test_001_multiturn": {
                        "conversation": [
                            {"role": "user", "content": "...", "audio_path": "..."},
                            {"role": "assistant", "content": "", "video_path": "..."},
                            ...
                        ]
                    }
                }
            image_path: å‚è€ƒå›¾åƒè·¯å¾„
            id: å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            Dict: åŒ…å«æ¯è½®ç”Ÿæˆç»“æœçš„å­—å…¸
        """
        print("\n" + "="*80)
        print("MULTI-TURN CONVERSATION PROCESSING")
        print("="*80)
        
        # æå–conversationåˆ—è¡¨
        # æ”¯æŒä¸¤ç§æ ¼å¼ï¼šç›´æ¥ä¼ conversationåˆ—è¡¨æˆ–åµŒå¥—åœ¨å­—å…¸ä¸­
        if isinstance(conversation_data, dict):
            if "conversation" in conversation_data:
                conversation = conversation_data["conversation"]
                if id is None:
                    id = list(conversation_data.keys())[0] if len(conversation_data) == 1 else "default"
            else:
                # å‡è®¾æ˜¯åµŒå¥—æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªkey
                first_key = list(conversation_data.keys())[0]
                conversation = conversation_data[first_key]["conversation"]
                if id is None:
                    id = first_key
        else:
            conversation = conversation_data
            if id is None:
                id = "default"
        
        print(f"ID: {id}")
        print(f"Total turns in conversation: {len(conversation)}")
        print(f"Reference image: {image_path}")
        
        # åˆå§‹åŒ–Qwen-Omni talker
        qwen_talker = QwenOmniTalker()
        
        # å­˜å‚¨æ¯è½®çš„ç»“æœ
        results = {
            "id": id,
            "turns": []
        }
        
        # éå†å¯¹è¯ï¼Œå¤„ç†æ¯ä¸ªuser-assistantå¯¹
        turn_number = 0
        for i in range(0, len(conversation), 2):
            if i + 1 >= len(conversation):
                print(f"Warning: Incomplete turn at index {i}, skipping")
                break
            
            user_turn = conversation[i]
            assistant_turn = conversation[i + 1]
            
            # éªŒè¯è§’è‰²
            if user_turn.get("role") != "user" or assistant_turn.get("role") != "assistant":
                print(f"Warning: Invalid roles at index {i}, skipping")
                continue
            
            turn_number += 1
            print(f"\n{'='*60}")
            print(f"Processing Turn {turn_number}")
            print(f"{'='*60}")
            
            # è·å–ç”¨æˆ·è¾“å…¥
            user_content = user_turn.get("content", "")
            user_audio_path = user_turn.get("audio_path")
            assistant_video_path = assistant_turn.get("video_path")
            
            if not user_audio_path:
                print(f"Warning: No audio_path in user turn {turn_number}, skipping")
                continue
            
            if not assistant_video_path:
                print(f"Warning: No video_path in assistant turn {turn_number}, skipping")
                continue
            
            print(f"User content: {user_content}")
            print(f"User audio: {user_audio_path}")
            print(f"Target video path: {assistant_video_path}")
            
            try:
                # Step 1: ä½¿ç”¨Qwen-Omniå¤„ç†éŸ³é¢‘ï¼Œç”Ÿæˆå›å¤éŸ³é¢‘å’Œæ–‡æœ¬
                print(f"\n[Turn {turn_number}] Step 1: Processing audio with Qwen-Omni...")
                reply_audio_path, reply_text = qwen_talker.process_audio_conversation(
                    audio_path=user_audio_path,
                    session_id=id,
                    prompt=user_content if user_content else "Please respond to this audio."
                )
                
                if reply_audio_path is None or reply_text is None:
                    print(f"Error: Failed to get reply from Qwen-Omni for turn {turn_number}")
                    results["turns"].append({
                        "turn": turn_number,
                        "status": "failed",
                        "error": "Qwen-Omni processing failed"
                    })
                    continue
                
                print(f"Reply audio saved: {reply_audio_path}")
                print(f"Reply text: {reply_text}")
                
                # Step 2: ä½¿ç”¨reply_audioç”Ÿæˆè§†é¢‘
                print(f"\n[Turn {turn_number}] Step 2: Generating video with audio...")
                temp_video_path = self._generate_video_for_turn(
                    image_path=image_path,
                    audio_path=reply_audio_path,
                    text_prompt=reply_text,
                    turn_number=turn_number,
                    id=id
                )
                
                if temp_video_path is None:
                    print(f"Error: Failed to generate video for turn {turn_number}")
                    results["turns"].append({
                        "turn": turn_number,
                        "status": "failed",
                        "error": "Video generation failed"
                    })
                    continue
                
                # Step 3: ç§»åŠ¨è§†é¢‘åˆ°ç›®æ ‡è·¯å¾„
                print(f"\n[Turn {turn_number}] Step 3: Moving video to target path...")
                os.makedirs(os.path.dirname(assistant_video_path), exist_ok=True)
                
                # å¦‚æœtemp_video_pathå’Œassistant_video_pathä¸åŒï¼Œåˆ™å¤åˆ¶/ç§»åŠ¨æ–‡ä»¶
                if os.path.abspath(temp_video_path) != os.path.abspath(assistant_video_path):
                    import shutil
                    shutil.move(temp_video_path, assistant_video_path)
                    print(f"Video moved to: {assistant_video_path}")
                else:
                    print(f"Video already at target path: {assistant_video_path}")
                
                # è®°å½•æˆåŠŸç»“æœ
                results["turns"].append({
                    "turn": turn_number,
                    "status": "success",
                    "user_audio": user_audio_path,
                    "reply_audio": reply_audio_path,
                    "reply_text": reply_text,
                    "video_path": assistant_video_path
                })
                
                print(f"\nâœ“ Turn {turn_number} completed successfully!")
                
            except Exception as e:
                print(f"\nâœ— Error processing turn {turn_number}: {e}")
                traceback.print_exc()
                results["turns"].append({
                    "turn": turn_number,
                    "status": "failed",
                    "error": str(e)
                })
        
        # æ‰“å°æ€»ç»“
        print("\n" + "="*80)
        print("MULTI-TURN CONVERSATION SUMMARY")
        print("="*80)
        successful_turns = sum(1 for t in results["turns"] if t["status"] == "success")
        print(f"Total turns processed: {len(results['turns'])}")
        print(f"Successful: {successful_turns}")
        print(f"Failed: {len(results['turns']) - successful_turns}")
        print("="*80 + "\n")
        
        return results
    
    def _generate_video_for_turn(
        self,
        image_path: str,
        audio_path: str,
        text_prompt: str,
        turn_number: int,
        id: str
    ) -> Optional[str]:
        """
        ä¸ºå•ä¸ªå¯¹è¯è½®æ¬¡ç”Ÿæˆè§†é¢‘ï¼ˆå†…éƒ¨è¾…åŠ©æ–¹æ³•ï¼‰
        
        Args:
            image_path: å‚è€ƒå›¾åƒè·¯å¾„
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            text_prompt: æ–‡æœ¬æç¤º
            turn_number: å½“å‰è½®æ¬¡ç¼–å·
            id: å”¯ä¸€æ ‡è¯†ç¬¦
        
        Returns:
            str: ç”Ÿæˆçš„è§†é¢‘è·¯å¾„ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        try:
            # å‡†å¤‡å™ªå£°è¾“å…¥
            batch_size = 1
            num_frames = self.args.num_frame_per_block * 3  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
            num_channels = 16
            height = self.args.latent_height
            width = self.args.latent_width
            
            noise = torch.randn(
                [batch_size, num_frames, num_channels, height, width],
                device=self.device,
                dtype=self.dtype
            )
            
            # è°ƒç”¨forwardæ–¹æ³•ç”Ÿæˆè§†é¢‘
            print(f"Generating video with {num_frames} frames...")
            self.forward(
                noise=noise,
                text_prompts=text_prompt,
                image_path=image_path,
                audio_path=audio_path,
                initial_latent=None,
                return_latents=False,
                id=f"{id}_turn_{turn_number}"
            )
            
            # è§†é¢‘å·²é€šè¿‡_save_complete_video_with_audioä¿å­˜
            # æŸ¥æ‰¾æœ€æ–°ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶
            output_dir = "output_videos"
            id_str = f"_{id}_turn_{turn_number}"
            
            # æŸ¥æ‰¾åŒ¹é…çš„è§†é¢‘æ–‡ä»¶
            video_files = [
                f for f in os.listdir(output_dir)
                if f.startswith("video_with_audio") and id_str in f
            ]
            
            if video_files:
                # è¿”å›æœ€æ–°çš„æ–‡ä»¶
                video_files.sort(reverse=True)
                video_path = os.path.join(output_dir, video_files[0])
                print(f"Generated video: {video_path}")
                return video_path
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°with_audioç‰ˆæœ¬ï¼ŒæŸ¥æ‰¾no_audioç‰ˆæœ¬
                video_files_no_audio = [
                    f for f in os.listdir(output_dir)
                    if f.startswith("video_no_audio") and id_str in f
                ]
                if video_files_no_audio:
                    video_files_no_audio.sort(reverse=True)
                    video_path = os.path.join(output_dir, video_files_no_audio[0])
                    print(f"Generated video (no audio merged): {video_path}")
                    return video_path
                else:
                    print("Error: Could not find generated video file")
                    return None
            
        except Exception as e:
            print(f"Error generating video: {e}")
            traceback.print_exc()
            return None


def batch_process_multiturn_conversations_from_json(
    pipeline,
    json_file_path: str,
    image_path: str
) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†JSONæ–‡ä»¶ä¸­çš„æ‰€æœ‰å¤šè½®å¯¹è¯æ•°æ®
    
    å‚è€ƒpipelined_websocket_streaming_server.pyä¸­çš„QwenOmniTalkerå®ç°ï¼Œ
    ä¸ºæ¯æ¡å¤šè½®å¯¹è¯æ•°æ®ç”Ÿæˆå¯¹åº”çš„è§†é¢‘å›å¤ã€‚
    
    Args:
        pipeline: PipelinedEvalPipelineå®ä¾‹
        json_file_path: JSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«å¤šæ¡å¤šè½®å¯¹è¯æ•°æ®
        image_path: å‚è€ƒå›¾åƒè·¯å¾„ï¼ˆæ‰€æœ‰å¯¹è¯å…±ç”¨ï¼‰
    
    Returns:
        Dict: åŒ…å«æ‰€æœ‰å¯¹è¯çš„å¤„ç†ç»“æœ
    
    JSONæ ¼å¼ç¤ºä¾‹ï¼š
    {
        "conversation_001": {
            "conversation": [
                {"role": "user", "content": "...", "audio_path": "..."},
                {"role": "assistant", "content": "", "video_path": "..."},
                ...
            ]
        },
        "conversation_002": {
            "conversation": [...]
        },
        ...
    }
    """
    print("\n" + "="*80)
    print(f"BATCH PROCESSING MULTI-TURN CONVERSATIONS FROM: {json_file_path}")
    print("="*80 + "\n")
    
    # è¯»å–JSONæ–‡ä»¶
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            all_conversations = json.load(f)
    except Exception as e:
        print(f"Error loading JSON file: {e}")
        traceback.print_exc()
        return {"error": str(e), "conversations": []}
    
    # éªŒè¯å›¾åƒè·¯å¾„
    if not os.path.exists(image_path):
        print(f"Error: Image path does not exist: {image_path}")
        return {"error": f"Image not found: {image_path}", "conversations": []}
    
    print(f"Image path: {image_path}")
    print(f"Total conversations found: {len(all_conversations)}\n")
    
    # å­˜å‚¨æ‰€æœ‰å¯¹è¯çš„å¤„ç†ç»“æœ
    batch_results = {
        "json_file": json_file_path,
        "image_path": image_path,
        "total_conversations": len(all_conversations),
        "conversations": []
    }
    
    # éå†æ¯æ¡å¯¹è¯æ•°æ®
    for conv_idx, (conv_id, conv_data) in enumerate(all_conversations.items(), 1):
        print(f"\n{'='*80}")
        print(f"Processing Conversation {conv_idx}/{len(all_conversations)}: {conv_id}")
        print(f"{'='*80}\n")
        
        try:
            # è°ƒç”¨process_multiturn_conversationå¤„ç†å•æ¡å¯¹è¯
            conv_result = pipeline.process_multiturn_conversation(
                conversation_data={conv_id: conv_data},
                image_path=image_path,
                id=conv_id
            )
            
            # è®°å½•ç»“æœ
            batch_results["conversations"].append({
                "id": conv_id,
                "status": "success",
                "result": conv_result
            })
            
            print(f"\nâœ“ Conversation {conv_id} processed successfully!")
            
        except Exception as e:
            print(f"\nâœ— Error processing conversation {conv_id}: {e}")
            traceback.print_exc()
            
            batch_results["conversations"].append({
                "id": conv_id,
                "status": "failed",
                "error": str(e)
            })
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*80)
    print("BATCH PROCESSING SUMMARY")
    print("="*80)
    successful = sum(1 for c in batch_results["conversations"] if c["status"] == "success")
    failed = len(batch_results["conversations"]) - successful
    print(f"Total conversations: {len(batch_results['conversations'])}")
    print(f"Successful: {successful}")
    print(f"Failed: {failed}")
    print("="*80 + "\n")
    
    # ä¿å­˜ç»“æœåˆ°JSON
    output_json_path = json_file_path.replace('.json', '_batch_results.json')
    try:
        with open(output_json_path, 'w', encoding='utf-8') as f:
            json.dump(batch_results, f, indent=2, ensure_ascii=False)
        print(f"Batch results saved to: {output_json_path}\n")
    except Exception as e:
        print(f"Warning: Failed to save batch results: {e}\n")
    
    return batch_results


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    
    python scripts/pipelined_eval_gen.py \\
        --json_path conversations.json \\
        --image_path reference.jpg \\
        --config configs/causal_inference.yaml
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='Batch process multi-turn conversations')
    parser.add_argument('--json_path', type=str, help='Path to JSON file containing conversations')
    parser.add_argument('--image_path', type=str, help='Path to reference image')
    parser.add_argument('--config', type=str, default='configs/causal_inference.yaml', help='Path to config file')
    
    cmd_args = parser.parse_args()
    
    if cmd_args.json_path and cmd_args.image_path:
        # æ‰¹é‡å¤„ç†æ¨¡å¼
        print("\n" + "="*80)
        print("BATCH PROCESSING MODE")
        print("="*80 + "\n")
        
        # è§£æé…ç½®
        args = parse_args()
        
        # åˆå§‹åŒ–pipeline
        print("Initializing pipeline...")
        pipeline = PipelinedEvalPipeline(args)
        print("Pipeline initialized successfully!\n")
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰å¯¹è¯
        results = batch_process_multiturn_conversations_from_json(
            pipeline=pipeline,
            json_file_path=cmd_args.json_path,
            image_path=cmd_args.image_path
        )
        
        print("\n" + "="*80)
        print("BATCH PROCESSING COMPLETE!")
        print("="*80 + "\n")
        
    else:
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        print("\n" + "="*80)
        print("PIPELINED EVAL GEN - Multi-Turn Conversation Batch Processing")
        print("="*80 + "\n")
        print("This script supports batch processing of multi-turn conversations from JSON files.")
        print("\nUsage:")
        print("  python scripts/pipelined_eval_gen.py \\")
        print("    --json_path conversations.json \\")
        print("    --image_path reference.jpg \\")
        print("    --config configs/causal_inference.yaml")
        print("\nJSON file format:")
        print("""
{
  "conversation_001": {
    "conversation": [
      {"role": "user", "content": "Hello", "audio_path": "audio1.wav"},
      {"role": "assistant", "content": "", "video_path": "video1.mp4"},
      {"role": "user", "content": "How are you?", "audio_path": "audio2.wav"},
      {"role": "assistant", "content": "", "video_path": "video2.mp4"}
    ]
  },
  "conversation_002": {
    "conversation": [...]
  }
}
""")
        print("="*80 + "\n")
    
