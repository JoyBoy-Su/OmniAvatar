#!/usr/bin/env python3

import os
import sys
import io
import math
import json
import base64
import tempfile
import traceback
import librosa
import copy
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import asyncio
import threading
import queue
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from flask import Flask, request
from flask_cors import CORS
from flask_socketio import SocketIO, emit, join_room, leave_room
from PIL import Image
import time
import numpy as np
import torchvision.transforms as transforms
from collections import deque
import subprocess
import cv2
from typing import List, Optional, Dict, Any
from collections import defaultdict
import soundfile as sf

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from peft import LoraConfig, inject_adapter_in_model, PeftModel

from OmniAvatar.utils.args_config import parse_args
from scripts.inference import match_size, resize_pad
from OmniAvatar.utils.io_utils import save_video_as_grid_and_mp4, load_state_dict
from OmniAvatar.distributed.fsdp import shard_model
import torch.distributed as dist
import torchvision.transforms as transforms
import torch.nn.functional as F
import torchvision.transforms as TT
from transformers import Wav2Vec2FeatureExtractor
from PIL import Image
from OmniAvatar.models.model_manager import ModelManager
from OmniAvatar.prompters import WanPrompter
from OmniAvatar.schedulers.flow_match import FlowMatchScheduler
from scripts.causal_inference import CausalInferencePipeline
from openai import OpenAI

class QwenOmniTalker:
    """Qwen-Omniè¯­éŸ³å¯¹è¯å¤„ç†å™¨"""
    
    def __init__(self, api_key="sk-63ad221681734d339b8171797204f105", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"):
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url,
        )
        self.system_message = {
            "role": "system",
            "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
        }
        
    def process_audio_conversation_stream(self, audio_path, session_id=None, prompt="Analyze this audio and respond naturally.", 
                                          fps=16, frames_per_block=12, sample_rate=24000):
        """
        æµå¼å¤„ç†éŸ³é¢‘å¯¹è¯ï¼Œä»¥blockä¸ºå•ä½yieldéŸ³é¢‘æ•°æ®

        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            session_id: ä¼šè¯IDï¼Œç”¨äºç”Ÿæˆå”¯ä¸€çš„è¾“å‡ºæ–‡ä»¶å
            prompt: æ–‡æœ¬æç¤ºè¯ï¼Œé»˜è®¤ä¸ºåˆ†æéŸ³é¢‘å†…å®¹
            fps: è§†é¢‘å¸§ç‡ï¼Œé»˜è®¤16
            frames_per_block: æ¯ä¸ªblockçš„å¸§æ•°ï¼Œé»˜è®¤12
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡ï¼Œé»˜è®¤24000

        Yields:
            tuple: (block_audio_array, block_text, is_final)
                - block_audio_array: numpyæ•°ç»„ï¼Œshapeä¸º(samples,)ï¼Œdtypeä¸ºint16
                - block_text: è¯¥blockå¯¹åº”çš„æ–‡æœ¬ç‰‡æ®µ
                - is_final: æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªblock
        """
        try:
            # è®¡ç®—æ¯ä¸ªblockçš„é‡‡æ ·ç‚¹æ•°
            block_duration = frames_per_block / fps  # 0.75ç§’
            samples_per_block = int(block_duration * sample_rate)  # 18000ä¸ªé‡‡æ ·ç‚¹
            bytes_per_block = samples_per_block * 2  # 36000å­—èŠ‚ï¼ˆint16ï¼‰
            
            # è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶ç¼–ç ä¸ºbase64
            with open(audio_path, 'rb') as audio_file:
                audio_bytes = audio_file.read()
                audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')

            # æ„å»ºæ¶ˆæ¯ - ä½¿ç”¨input_audioæ ¼å¼å‘é€éŸ³é¢‘è¾“å…¥
            messages = [
                self.system_message,
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "input_audio", "input_audio": {"data": f"data:;base64,{audio_base64}", "format": "wav"}},
                    ],
                },
            ]
            
            # è°ƒç”¨Qwen-Omni API
            completion = self.client.chat.completions.create(
                model="qwen3-omni-flash",
                messages=messages,
                modalities=["text", "audio"],
                audio={
                    "voice": "Cherry",  # Cherry, Ethan, Serena, Chelsie is available
                    "format": "wav"
                },
                stream=True,
                stream_options={"include_usage": True}
            )
            
            # æµå¼æ”¶é›†å’Œè¾“å‡ºå“åº”
            text_parts = []
            audio_buffer = b""  # å­—èŠ‚ç¼“å†²åŒº
            block_idx = 0
            
            for chunk in completion:
                if chunk.choices:
                    if hasattr(chunk.choices[0].delta, "audio") and chunk.choices[0].delta.audio:
                        try:
                            if "data" in chunk.choices[0].delta.audio:
                                # ç´¯ç§¯éŸ³é¢‘æ•°æ®
                                chunk_audio_base64 = chunk.choices[0].delta.audio["data"]
                                chunk_audio_bytes = base64.b64decode(chunk_audio_base64)
                                audio_buffer += chunk_audio_bytes
                                
                                # å½“ç´¯ç§¯è¶³å¤Ÿä¸€ä¸ªblockæ—¶ï¼Œyieldå‡ºå»
                                while len(audio_buffer) >= bytes_per_block:
                                    block_bytes = audio_buffer[:bytes_per_block]
                                    audio_buffer = audio_buffer[bytes_per_block:]
                                    
                                    block_audio_array = np.frombuffer(block_bytes, dtype=np.int16)
                                    block_text = "".join(text_parts) if text_parts else ""
                                    
                                    print(f"Yielding block {block_idx}: {len(block_audio_array)} samples, text_len={len(block_text)}")
                                    yield (block_audio_array, block_text, False)
                                    block_idx += 1
                                    text_parts = []  # æ¸…ç©ºå·²è¾“å‡ºçš„æ–‡æœ¬
                                    
                            elif "transcript" in chunk.choices[0].delta.audio:
                                text_parts.append(chunk.choices[0].delta.audio["transcript"])
                        except Exception as e:
                            print(f"Error processing audio chunk: {e}")
                    elif hasattr(chunk.choices[0].delta, "content") and chunk.choices[0].delta.content:
                        text_parts.append(chunk.choices[0].delta.content)
                else:
                    if hasattr(chunk, 'usage') and chunk.usage:
                        print(f"Usage: {chunk.usage}")
            
            # å¤„ç†å‰©ä½™çš„éŸ³é¢‘æ•°æ®ï¼ˆæœ€åä¸€ä¸ªä¸å®Œæ•´çš„blockï¼‰
            if len(audio_buffer) > 0:
                # å¡«å……åˆ°å®Œæ•´blockï¼ˆç”¨é™éŸ³å¡«å……ï¼‰
                padding_size = bytes_per_block - len(audio_buffer)
                if padding_size > 0:
                    audio_buffer += b'\x00' * padding_size
                
                block_audio_array = np.frombuffer(audio_buffer[:bytes_per_block], dtype=np.int16)
                block_text = "".join(text_parts) if text_parts else ""
                
                print(f"Yielding final block {block_idx}: {len(block_audio_array)} samples (padded), text_len={len(block_text)}")
                yield (block_audio_array, block_text, True)
            else:
                # å¦‚æœæ²¡æœ‰å‰©ä½™æ•°æ®ï¼Œå‘é€ç©ºçš„finalæ ‡è®°
                if block_idx > 0:
                    print(f"All blocks completed, total {block_idx} blocks")
                else:
                    print("Warning: No audio data received from Qwen-Omni")
                    yield (np.array([], dtype=np.int16), "".join(text_parts), True)
                
        except Exception as e:
            print(f"Error in Qwen-Omni conversation stream: {e}")
            traceback.print_exc()
            yield (None, None, True)

class BufferedGenerator:
    """
    Buffered Generator for audio conversation stream
    å¯åŠ¨æ—¶ç«‹å³å¼€å§‹åœ¨åå°çº¿ç¨‹ä¸­ç”Ÿæˆæ•°æ®ï¼Œæä¾›çœŸæ­£çš„æµå¼ä½“éªŒ
    """
    def __init__(self, generator_func, start_immediately=True):
        self.generator_func = generator_func
        self.queue = queue.Queue()  # ä»»åŠ¡é˜Ÿåˆ—
        self.stop_event = threading.Event()
        self.producer_thread = None
        
        # ç«‹å³å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹ï¼Œæå‰å¼€å§‹ç”Ÿæˆ
        if start_immediately:
            self._start_producer()
        
    def _start_producer(self):
        """å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹"""
        if self.producer_thread is None or not self.producer_thread.is_alive():
            self.producer_thread = threading.Thread(target=self._producer, daemon=True)
            self.producer_thread.start()
            print("BufferedGenerator: Producer thread started")
        
    def _producer(self):
        """ç”Ÿäº§è€…çº¿ç¨‹ï¼šåœ¨åå°ç”Ÿæˆæ•°æ®å¹¶æ”¾å…¥é˜Ÿåˆ—"""
        try:
            for item in self.generator_func():
                if self.stop_event.is_set():
                    print("BufferedGenerator: Stop event set, exiting producer")
                    break
                self.queue.put(item)
            self.queue.put(None)  # ç»“æŸæ ‡è®°
            print("BufferedGenerator: Producer finished")
        except Exception as e:
            print(f"BufferedGenerator: Producer error: {e}")
            traceback.print_exc()
            self.queue.put(None)
    
    def __iter__(self):
        return self
    
    def __next__(self):
        # å¦‚æœçº¿ç¨‹è¿˜æ²¡å¯åŠ¨ï¼Œå¯åŠ¨å®ƒï¼ˆå…œåº•ä¿æŠ¤ï¼‰
        if self.producer_thread is None or not self.producer_thread.is_alive():
            self._start_producer()
        
        # ä»é˜Ÿåˆ—ä¸­è·å–æ•°æ®ï¼ˆå¦‚æœæ•°æ®å·²ç”Ÿæˆåˆ™ç«‹å³è¿”å›ï¼Œå¦åˆ™é˜»å¡ç­‰å¾…ï¼‰
        item = self.queue.get()
        if item is None:
            if self.producer_thread.is_alive():
                self.producer_thread.join(timeout=1.0)
            raise StopIteration
        return item

    def stop(self):
        self.stop_event.set()
        if self.producer_thread is not None and self.producer_thread.is_alive():
            self.producer_thread.join()

class PipelinedConversation(nn.Module):
    """
    æµæ°´çº¿åŒ–çš„Conversation Pipelineå®ç°
    """
    
    def __init__(self, args):
        super().__init__()
        self.args = args
        print(f"self.args: {self.args}")
        # set device and dtype
        self.device = torch.device(f"cuda:{args.rank}")
        if args.dtype=='bf16':
            self.dtype = torch.bfloat16
        elif args.dtype=='fp16':
            self.dtype = torch.float16
        else:   
            self.dtype = torch.float32
        
        # load causal inference pipeline
        self.causal_pipe = self.load_causal_model()
        
        # talker
        self.talker: QwenOmniTalker = QwenOmniTalker()
        
        # æµæ°´çº¿ç›¸å…³å‚æ•°
        self.audio_gen_buffer = None
        self.accumulated_audio = np.array([], dtype=np.int16)
        self.accumulated_audio_lock = threading.Lock()
        self.denoising_queue = queue.Queue()  # denoisingä»»åŠ¡é˜Ÿåˆ—
        self.vae_queue = queue.Queue()  # decodingä»»åŠ¡é˜Ÿåˆ—
        self.result_buffer = {}  # ä¿æŒå­—å…¸ç»“æ„ï¼Œç”¨äºæŒ‰chunk_idæ’åº
        # å¤šçº¿ç¨‹æ§åˆ¶
        self.denoising_thread = None
        self.vae_thread = None
        self.result_lock = threading.Lock()
        self.stop_workers = threading.Event()  # Signal to stop worker threads
        # äº‹ä»¶è§¦å‘åŒæ­¥æœºåˆ¶
        self.denoising_events = deque()
        self.vae_events = deque()
        self.current_clock = 0
        
        # æ€§èƒ½ç»Ÿè®¡ç›¸å…³
        self.timing_stats = defaultdict(list)  # å­˜å‚¨å„æ­¥éª¤çš„è€—æ—¶ç»Ÿè®¡
        self.timing_lock = threading.Lock()  # ä¿æŠ¤timing_statsçš„çº¿ç¨‹é”
    
    def load_causal_model(self):
        """Load causal inference pipeline"""
        # Initialize model manager
        model_manager = ModelManager(device="cpu", infer=True)
        model_manager.load_models(
            [
                self.args.dit_path.split(","),   # load dit
                self.args.text_encoder_path,     # load text encoder
                self.args.vae_path               # load vae
            ],
            torch_dtype=self.dtype,
            device='cpu',
        )
        
        # Create causal inference pipeline
        causal_pipe = CausalInferencePipeline.from_model_manager(
            model_manager=model_manager,
            args=self.args,
            device=self.device
        )
        
        # Move VAE to different GPU for pipeline parallelism
        causal_pipe.vae.to("cuda:1")
        print(f"Move VAE to cuda:1 for pipeline parallelism")
        
        return causal_pipe
    
    @torch.no_grad()
    def encode_audio_block(self, audio_block, sr=16000, block_idx=0, 
                          is_first_block=False, is_last_block=False,
                          fps=16, frames_per_block=12, h=1):
        """
        ç¼–ç å•ä¸ªéŸ³é¢‘blockï¼Œè¾“å‡ºå¯¹åº”çš„éŸ³é¢‘ç‰¹å¾
        
        å¤ç”¨ causal_inference_audio_stream.py çš„é€»è¾‘ï¼š
        - ç¬¬0å—ï¼šç¼–ç 9å¸§ï¼Œå¤åˆ¶ç¬¬0å¸§3æ¬¡ï¼Œè¾“å‡º12å¸§
        - å—1-6ï¼šç¼–ç 12+2å¸§ï¼ˆå·¦å³å„1å¸§overlapï¼‰ï¼Œdiscardåè¾“å‡º12å¸§
        
        Args:
            audio_block: numpyæ•°ç»„ [audio_samples]ï¼Œint16æ ¼å¼ï¼Œé‡‡æ ·ç‡sr
            sr: éŸ³é¢‘é‡‡æ ·ç‡ï¼Œé»˜è®¤24000ï¼ˆQwen-Omniè¾“å‡ºï¼‰
            block_idx: å½“å‰å—ç´¢å¼•ï¼ˆ0-6ï¼‰
            is_first_block: æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªå—
            is_last_block: æ˜¯å¦æ˜¯æœ€åä¸€ä¸ªå—
            fps: è§†é¢‘å¸§ç‡ï¼Œé»˜è®¤16
            frames_per_block: æ¯å—è¾“å‡ºçš„å¸§æ•°ï¼Œé»˜è®¤12
            h: overlapåŒºåŸŸå¤§å°ï¼Œé»˜è®¤1å¸§
            
        Returns:
            audio_emb: torch.Tensor, shape=[1, frames_per_block, 10752]
        """
        frame_duration = 1.0 / fps  # æ¯å¸§æ—¶é•¿ï¼ˆç§’ï¼‰
        
        # å°†éŸ³é¢‘ä»int16è½¬æ¢ä¸ºfloat32ï¼ˆå½’ä¸€åŒ–åˆ°[-1, 1]ï¼‰
        if audio_block.dtype == np.int16:
            audio = audio_block.astype(np.float32) / 32768.0
        else:
            audio = audio_block.astype(np.float32)
        
        # å¦‚æœé‡‡æ ·ç‡ä¸æ˜¯16000ï¼Œéœ€è¦é‡é‡‡æ ·
        if sr != 16000:
            import librosa
            audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)
            sr = 16000
        print(f"encode_audio_block - Block {block_idx} (accumulated mode): audio length {len(audio)} samples")
        
        # è®¡ç®—å½“å‰å—å¯¹åº”çš„å¸§èŒƒå›´
        if is_first_block:
            # ç¬¬0å—ç‰¹æ®Šå¤„ç†ï¼šåªç¼–ç å¸§0-8ï¼ˆ9å¸§ï¼‰
            start_frame = 0
            end_frame = 9
            actual_frames = 9
            
            # æ·»åŠ ä¸Šä¸‹æ–‡å¸§ï¼ˆç¬¬0å—åªæœ‰å³ä¾§ä¸Šä¸‹æ–‡ï¼‰
            start_frame_with_context = 0
            end_frame_with_context = end_frame + h
            target_seq_len = actual_frames + h  # 9 + 1 = 10
        else:
            # åç»­å—æ­£å¸¸å¤„ç†
            start_frame = 9 + (block_idx - 1) * frames_per_block
            end_frame = start_frame + frames_per_block
            actual_frames = frames_per_block
            
            # æ·»åŠ ä¸Šä¸‹æ–‡å¸§
            start_frame_with_context = max(0, start_frame - h)
            if is_last_block:
                end_frame_with_context = end_frame
                target_seq_len = actual_frames + h  # 12 + 1 = 13
            else:
                end_frame_with_context = end_frame + h
                target_seq_len = actual_frames + 2 * h  # 12 + 2 = 14
        
        # è®¡ç®—å¯¹åº”çš„éŸ³é¢‘èŒƒå›´ï¼ˆé‡‡æ ·ç‚¹ï¼‰
        start_sample = int(start_frame_with_context * frame_duration * sr)
        end_sample = int(end_frame_with_context * frame_duration * sr)
        
        # å¯¹äºç´¯ç§¯éŸ³é¢‘ï¼Œç¡®ä¿ä¸è¶…å‡ºéŸ³é¢‘é•¿åº¦
        audio_length = len(audio)
        start_sample = min(start_sample, audio_length)
        end_sample = min(end_sample, audio_length)
        
        # æå–éŸ³é¢‘ç‰‡æ®µ
        audio_segment = audio[start_sample:end_sample]
        
        # å¦‚æœéŸ³é¢‘ç‰‡æ®µå¤ªçŸ­ï¼Œè¿›è¡Œå¡«å……
        min_audio_length = int(0.1 * sr)  # è‡³å°‘0.1ç§’
        if len(audio_segment) < min_audio_length:
            audio_segment = np.pad(audio_segment, (0, min_audio_length - len(audio_segment)),
                                   mode='constant', constant_values=0)
            print(f"encode_audio_block - Block {block_idx}: padded audio segment to {len(audio_segment)} samples")
        # ç¼–ç éŸ³é¢‘ç‰‡æ®µ
        input_values = np.squeeze(
            self.causal_pipe.wav_feature_extractor(audio_segment, sampling_rate=sr).input_values
        )
        input_values = torch.from_numpy(input_values).float().to(device=self.device)
        input_values = input_values.unsqueeze(0)
        
        # ä½¿ç”¨audio_encoderç¼–ç 
        with torch.no_grad():
            self.causal_pipe.audio_encoder.to(self.device)
            hidden_states = self.causal_pipe.audio_encoder(
                input_values, 
                seq_len=target_seq_len, 
                output_hidden_states=True
            )
            audio_embeddings = hidden_states.last_hidden_state
            for mid_hidden_states in hidden_states.hidden_states:
                audio_embeddings = torch.cat((audio_embeddings, mid_hidden_states), -1)
        
        # audio_embeddings shape: (1, target_seq_len, 10752)
        # Overlap-discard: æ ¹æ®è¾¹ç•Œæƒ…å†µé€‰æ‹©æ€§ä¸¢å¼ƒ
        if h > 0:
            if is_first_block:
                # ç¬¬0å—ï¼šåªä¸¢å¼ƒå³ä¾§hå¸§
                audio_embeddings_trimmed = audio_embeddings[:, :-h, :]  # (1, 9, 10752)
            elif is_last_block:
                # æœ€åä¸€å—ï¼šåªä¸¢å¼ƒå·¦ä¾§hå¸§
                audio_embeddings_trimmed = audio_embeddings[:, h:, :]  # (1, 12, 10752)
            else:
                # ä¸­é—´å—ï¼šä¸¢å¼ƒå‰åå„hå¸§
                audio_embeddings_trimmed = audio_embeddings[:, h:-h, :]  # (1, 12, 10752)
        else:
            audio_embeddings_trimmed = audio_embeddings
        
        # ç¬¬0å—ç‰¹æ®Šå¤„ç†ï¼šå°†ç¬¬0å¸§å¤åˆ¶3æ¬¡ï¼Œä½¿å…¶è¾“å‡º12å¸§
        if is_first_block:
            # audio_embeddings_trimmed shape: (1, 9, 10752)
            # å¤åˆ¶ç¬¬0å¸§3æ¬¡: [feat_0, feat_0, feat_0, feat_0, feat_1, ..., feat_8]
            first_frame = audio_embeddings_trimmed[:, 0:1, :]  # (1, 1, 10752)
            repeated_first_frame = first_frame.repeat(1, 3, 1)  # (1, 3, 10752)
            # æ‹¼æ¥ï¼š3ä¸ªé‡å¤ + åŸå§‹9å¸§ = 12å¸§
            audio_embeddings_trimmed = torch.cat([repeated_first_frame, audio_embeddings_trimmed], dim=1)
            print(f"encode_audio_block - Block {block_idx} (first): output shape {audio_embeddings_trimmed.shape}")
        else:
            print(f"encode_audio_block - Block {block_idx}: output shape {audio_embeddings_trimmed.shape}")
        
        # è½¬æ¢ä¸ºDiTæ‰€éœ€çš„æ ¼å¼ï¼š[1, 10752, 12, 1, 1]
        # æ³¨æ„ï¼š12å¸§å·²ç»èƒ½è¢«patch_size=4æ•´é™¤ï¼Œä¸éœ€è¦é¢å¤–å¤åˆ¶
        audio_emb = audio_embeddings_trimmed.permute(0, 2, 1)[:, :, :, None, None]
        # shape: [1, 10752, 12, 1, 1]
        
        # æŠ•å½±åˆ°DiTç‰¹å¾ç©ºé—´
        audio_emb = self.causal_pipe.generator.audio_proj(audio_emb.to(self.dtype))
        audio_emb = torch.concat([audio_cond_proj(audio_emb) for audio_cond_proj in self.causal_pipe.generator.audio_cond_projs], 0)
        
        print(f"encode_audio_block - Block {block_idx}: final audio_emb shape {audio_emb.shape}")
        
        return audio_emb
    
    def run_pipeline(
            self,
            noise: torch.Tensor,
            batch_size: int,
            num_blocks: int,
            num_input_frames: int,
            initial_latent: torch.Tensor,
            conditional_dict: dict,
            img_lat: torch.Tensor,
            output: torch.Tensor,
            audio_path: str,    # quesiton audio
            streaming_callback,
            session_id
        ):
        
        # Stop old worker threads if they exist
        if self.denoising_thread is not None or self.vae_thread is not None:
            print("Stopping old worker threads...")
            self.stop_workers.set()
            if self.denoising_thread is not None and self.denoising_thread.is_alive():
                self.denoising_thread.join(timeout=2.0)
            if self.vae_thread is not None and self.vae_thread.is_alive():
                self.vae_thread.join(timeout=2.0)
            self.stop_workers.clear()
            print("Old worker threads stopped")
        
        # clear pipeline state
        self.current_clock = 0
        self.denoising_queue.queue.clear()
        self.vae_queue.queue.clear()
        self.result_buffer.clear()
        self.denoising_events.clear()
        self.vae_events.clear()
        with self.accumulated_audio_lock:
            self.accumulated_audio = np.array([], dtype=np.int16)
            print(f"Reset accumulated audio, length: {len(self.accumulated_audio)}")
        
        # æ¸…ç©ºæ€§èƒ½ç»Ÿè®¡
        with self.timing_lock:
            self.timing_stats.clear()
        
        # run async threads
        # audio gen
        def audio_gen_func():
            return self.talker.process_audio_conversation_stream(audio_path, session_id)
        self.audio_gen_buffer = BufferedGenerator(audio_gen_func)
        self.denoising_thread = threading.Thread(target=self._causal_denoising_worker, daemon=True)
        self.vae_thread = threading.Thread(target=self._vae_worker, daemon=True)
        self.denoising_thread.start()
        self.vae_thread.start()
        
        # Step 2: cache context feature
        current_start_frame = 0
        if initial_latent is not None:
            print("INITIAL_LATENT is not None!!")
            raise ValueError
        
        # Step 3: temporal denoising loop
        print(f"NUM BLOCKS is {num_blocks}")
        all_num_frames = [self.args.num_frame_per_block] * num_blocks
        executor = ThreadPoolExecutor(max_workers=2)
        total_frames_generated = 0
        send_future = None
        
        # Process video blocks (limited by num_blocks)
        for video_block_idx in range(num_blocks):
            current_num_frames = all_num_frames[video_block_idx]
            print(f"===================== Current clock: {self.current_clock} ======================")
            print(f"Processing frame {current_start_frame - num_input_frames} to {current_start_frame + current_num_frames - num_input_frames}.")
            
            # get audio block
            try:
                audio_block, text, is_final = next(self.audio_gen_buffer)
                print(f"Received audio block {video_block_idx}: is_final={is_final}, samples={len(audio_block) if audio_block is not None else 0}")
                
                # update accumulated audio
                if audio_block is not None and len(audio_block) > 0:
                    with self.accumulated_audio_lock:
                        self.accumulated_audio = np.concatenate([self.accumulated_audio, audio_block])
                        print(f"Accumulated audio length: {len(self.accumulated_audio)} samples")
                
                audio_emb = self.encode_audio_block(audio_block, sr=24000, block_idx=self.current_clock, is_first_block=self.current_clock == 0, is_last_block=self.current_clock == num_blocks - 1, fps=16, frames_per_block=12, h=1)
            except StopIteration:
                print(f"Audio generation completed at block {video_block_idx}")
                break
            noisy_input = noise[:, current_start_frame - num_input_frames:current_start_frame + current_num_frames - num_input_frames]
            y_input = conditional_dict["image"][:, :, current_start_frame - num_input_frames:current_start_frame + current_num_frames - num_input_frames]
            audio_input = audio_emb.clone()
            block_conditional_dict = conditional_dict.copy()
            block_conditional_dict.update(image=y_input.clone(), audio=audio_input.clone())
            
            denoising_task = {
                "chunk_id": self.current_clock,
                "current_start_frame": current_start_frame,
                "current_num_frames": current_num_frames,
                "img_lat": img_lat.clone(),
                "batch_size": batch_size,
                "noisy_input": noisy_input.clone(),
                "block_conditional_dict": block_conditional_dict,
                "output": output.clone(),
                "audio_segment": audio_block
            }
            self.denoising_queue.put(denoising_task)
            # wait denoising clock and decoding clock - 1
            self.denoising_events.append(threading.Event())
            self.denoising_events[self.current_clock].wait()
            if self.current_clock >= 1:
                self.vae_events[self.current_clock - 1].wait()
            # send current chunk, TODO: update interface
            if send_future is not None:
                print("Waiting previous send task...")
                total_frames_generated = send_future.result()
                send_future = None
            send_future = executor.submit(
                self._send_chunk_frames,
                self.current_clock - 1,
                streaming_callback,
                session_id,
                total_frames_generated,
                num_blocks
            )
            # total_frames_generated = self._send_chunk_frames(self.current_clock - 1, streaming_callback, session_id, total_frames_generated, num_blocks)
            # import pdb; pdb.set_trace()
            # Step 3.4: update the start and end frame indices
            current_start_frame += current_num_frames
            self.current_clock += 1
        # Collect remaining audio blocks (if any)
        print("Collecting remaining audio blocks...")
        remaining_audio_blocks = 0
        try:
            while True:
                audio_block, text, is_final = next(self.audio_gen_buffer)
                if audio_block is not None and len(audio_block) > 0:
                    with self.accumulated_audio_lock:
                        self.accumulated_audio = np.concatenate([self.accumulated_audio, audio_block])
                        remaining_audio_blocks += 1
                        print(f"Collected remaining audio block {remaining_audio_blocks}: {len(audio_block)} samples")
                if is_final:
                    print(f"Finished collecting {remaining_audio_blocks} remaining audio blocks")
                    break
        except StopIteration:
            print(f"All audio blocks collected (total remaining: {remaining_audio_blocks})")
        
        print(f"Total accumulated audio: {len(self.accumulated_audio)} samples")
        
        # TODO: wait decoding the last chunk?
        self.vae_events[self.current_clock - 1].wait()
        if send_future is not None:
            print("Waiting previous send task...")
            total_frames_generated = send_future.result()
            send_future = None
        # send the last chunk, TODO: update interface
        total_frames_generated = self._send_chunk_frames(self.current_clock - 1, streaming_callback, session_id, total_frames_generated, num_blocks)
        
        # æ‰“å°æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š
        self._print_timing_report()
        # ä¿å­˜å®Œæ•´è§†é¢‘å¹¶åˆå¹¶éŸ³é¢‘
        self._save_complete_video_with_audio(num_blocks, audio_path, session_id)
  
    def _record_timing(self, step_name: str, duration: float, chunk_id: int = None):
        """è®°å½•æ­¥éª¤è€—æ—¶"""
        with self.timing_lock:
            key = f"{step_name}_chunk_{chunk_id}" if chunk_id is not None else step_name
            self.timing_stats[key].append(duration)
    
    def _print_timing_report(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("PERFORMANCE TIMING REPORT")
        print("="*80)
        
        with self.timing_lock:
            # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
            forward_steps = {}
            denoising_steps = {}
            vae_steps = {}
            send_steps = {}
            
            for key, times in self.timing_stats.items():
                if key.startswith('forward_'):
                    forward_steps[key] = times
                elif 'denoising' in key or 'generator_forward' in key:
                    denoising_steps[key] = times
                elif 'vae' in key or 'decode' in key:
                    vae_steps[key] = times
                elif 'send' in key:
                    send_steps[key] = times
            
            # æ‰“å°Forwardé˜¶æ®µç»Ÿè®¡
            if forward_steps:
                print("\nğŸ“Š FORWARD PHASE TIMING:")
                print("-" * 50)
                total_forward_time = 0
                for step, times in forward_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_forward_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL FORWARD TIME':<35}: {total_forward_time:>8.3f}s")
            
            # æ‰“å°Denoisingé˜¶æ®µç»Ÿè®¡
            if denoising_steps:
                print("\nğŸ”„ DENOISING PHASE TIMING:")
                print("-" * 50)
                total_denoising_time = 0
                for step, times in denoising_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_denoising_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL DENOISING TIME':<35}: {total_denoising_time:>8.3f}s")
            
            # æ‰“å°VAEé˜¶æ®µç»Ÿè®¡
            if vae_steps:
                print("\nğŸ¬ VAE DECODE PHASE TIMING:")
                print("-" * 50)
                total_vae_time = 0
                for step, times in vae_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_vae_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL VAE TIME':<35}: {total_vae_time:>8.3f}s")

            if send_steps:
                print("\nğŸ“¤ SEND PHASE TIMING:")
                print("-" * 50)
                total_send_time = 0
                for step, times in send_steps.items():
                    avg_time = np.mean(times)
                    total_time = np.sum(times)
                    total_send_time += total_time
                    print(f"  {step:<35}: {avg_time:>8.3f}s (total: {total_time:>8.3f}s, count: {len(times)})")
                print(f"  {'TOTAL SEND TIME':<35}: {total_send_time:>8.3f}s")
            
            # æ‰“å°æ€»ä½“ç»Ÿè®¡
            print("\nğŸ“ˆ OVERALL STATISTICS:")
            print("-" * 50)
            total_pipeline_time = sum(np.sum(times) for times in self.timing_stats.values())
            print(f"  {'TOTAL PIPELINE TIME':<35}: {total_pipeline_time:>8.3f}s")
            
            # åˆ†æç“¶é¢ˆ
            print("\nğŸ” BOTTLENECK ANALYSIS:")
            print("-" * 50)
            step_totals = {step: np.sum(times) for step, times in self.timing_stats.items()}
            sorted_steps = sorted(step_totals.items(), key=lambda x: x[1], reverse=True)
            for i, (step, total_time) in enumerate(sorted_steps[:5]):
                percentage = (total_time / total_pipeline_time) * 100 if total_pipeline_time > 0 else 0
                print(f"  {i+1}. {step:<30}: {total_time:>8.3f}s ({percentage:>5.1f}%)")
        
        print("="*80)

    @torch.no_grad()
    def forward(
        self,
        noise: torch.Tensor,
        text_prompts: str,
        image_path: Optional[str] = None,
        audio_path: Optional[str] = None,   # question audio path
        initial_latent: Optional[torch.Tensor] = None,
        return_latents: bool = False,
        streaming_callback=None,  # æµå¼ç”Ÿæˆå›è°ƒå‡½æ•°
        session_id=None
    ) -> torch.Tensor:
        # Calculate required number of frames from noise tensor
        batch_size, num_frames, num_channels, height, width = noise.shape

        # prepare image condition
        if image_path is not None:
            start_time = time.time()
            from PIL import Image
            image = Image.open(image_path).convert("RGB")
            image = self.causal_pipe.transform(image).unsqueeze(0).to(self.device)
            _, _, h, w = image.shape
            select_size = match_size(getattr(self.args, f'image_sizes_{self.args.max_hw}'), h, w)
            image = resize_pad(image, (h, w), select_size)
            image = image * 2.0 - 1.0
            image = image[:, :, None]
            image_prep_time = time.time() - start_time
            self._record_timing("forward_image_preprocessing", image_prep_time)
            
            start_time = time.time()
            self.causal_pipe.vae.to("cuda:1")
            # Use num_frames from noise tensor instead of hardcoded 21
            img_lat = self.causal_pipe.vae.encode(videos=image.to(dtype=self.dtype),device="cuda:1").repeat(1,1,num_frames,1,1)
            msk = torch.zeros_like(img_lat)[:,:1]
            msk[:, :, 1:] = 1
            img_lat = torch.cat([img_lat, msk], dim=1)
            image_encode_time = time.time() - start_time
            self._record_timing("forward_image_vae_encode", image_encode_time)
            print("img_lat:",img_lat.shape)

        # inference (prepare for run_pipeline)
        batch_size, num_frames, num_channels, height, width = noise.shape
        # frame block calculations
        assert num_frames % self.args.num_frame_per_block == 0
        num_blocks = num_frames // self.args.num_frame_per_block
        num_input_frames = initial_latent.shape[1] if initial_latent is not None else 0
        num_output_frames = num_frames + num_input_frames
        # text conditioning
        self.causal_pipe.text_encoder.to("cuda")
        self.causal_pipe.audio_encoder.to("cuda")
        self.causal_pipe.vae.clear_cache()
        conditional_dict = self.causal_pipe.encode_text_prompts(text_prompts, positive=True)
        conditional_dict["image"] = img_lat
        # TODO: add audio embeddings

        output = torch.zeros(
            [batch_size, num_output_frames, num_channels, height, width],
            device=noise.device,
            dtype=noise.dtype
        )
        # initialize KV caches
        self.causal_pipe.setup_caches(batch_size, noise.dtype, noise.device)
        print("Pipelined conversation init done")
        self.run_pipeline(
            noise, 
            batch_size,
            num_blocks,
            num_input_frames,
            initial_latent,
            conditional_dict,
            img_lat,
            output,
            audio_path,
            streaming_callback,
            session_id
        )
    
    @torch.no_grad()
    def _causal_denoising_worker(self):
        """
        Causal denoising worker
        ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡å¹¶æ‰§è¡Œå› æœæ¨ç†
        """
        while True:
            try:
                task = self.denoising_queue.get(timeout=0.1)
                chunk_id = task["chunk_id"]
                print(f"Processing causal denoising inference for chunk {chunk_id}")
                
                # è®°å½•æ•´ä¸ªdenoisingä»»åŠ¡çš„å¼€å§‹æ—¶é—´
                denoising_start_time = time.time()
                
                current_start_frame = task["current_start_frame"]
                current_num_frames = task["current_num_frames"]
                img_lat = task["img_lat"]
                batch_size = task["batch_size"]
                noisy_input = task["noisy_input"]
                block_conditional_dict = task["block_conditional_dict"]
                output = task["output"]
                # import pdb; pdb.set_trace()
                # Step 3.1: Spatial denoising loop
                denoising_loop_start = time.time()
                for index, current_timestep in enumerate(self.causal_pipe.denoising_step_list):
                    step_start_time = time.time()
                    
                    if current_start_frame == 0:
                        noisy_input[:, :1] = img_lat[:, :16, :1].permute(0, 2, 1, 3, 4)
                    timestep = torch.ones([batch_size, current_num_frames], device=noisy_input.device, dtype=torch.int64) * current_timestep
                    
                    # generate
                    generator_start_time = time.time()
                    v, denoised_pred = self.causal_pipe.generator_forward(
                        noisy_image_or_video=noisy_input,
                        conditional_dict=block_conditional_dict,
                        timestep=timestep,
                        kv_cache=self.causal_pipe.kv_cache1,
                        crossattn_cache=self.causal_pipe.crossattn_cache,
                        current_start=current_start_frame * self.causal_pipe.frame_seq_length
                    )
                    generator_time = time.time() - generator_start_time
                    self._record_timing(f"denoising_generator_forward_step_{index}", generator_time, chunk_id)
                    
                    if index < len(self.causal_pipe.denoising_step_list) - 1:
                        noise_start_time = time.time()
                        next_timestep = self.causal_pipe.denoising_step_list[index + 1]
                        noisy_input = self.causal_pipe.scheduler.add_noise(
                            denoised_pred.flatten(0, 1),
                            torch.randn_like(denoised_pred.flatten(0, 1)),
                            next_timestep * torch.ones([batch_size * current_num_frames], device=noisy_input.device, dtype=torch.long)
                        ).unflatten(0, denoised_pred.shape[:2])
                        noise_time = time.time() - noise_start_time
                        self._record_timing(f"denoising_add_noise_step_{index}", noise_time, chunk_id)
                    
                    step_time = time.time() - step_start_time
                    self._record_timing(f"denoising_total_step_{index}", step_time, chunk_id)
                
                denoising_loop_time = time.time() - denoising_loop_start
                self._record_timing("denoising_loop_total", denoising_loop_time, chunk_id)
                
                # Step 3.2: record the model's output
                if current_start_frame == 0:
                    denoised_pred[:, :1] = img_lat[:, :16, :1].permute(0, 2, 1, 3, 4)
                output[:, current_start_frame:current_start_frame + current_num_frames] = denoised_pred # latents: denoised_pred
                
                # Step 3.3: return with timestep zero to update KV cache using clean context
                context_start_time = time.time()
                context_timestep = torch.ones_like(timestep) * 0
                self.causal_pipe.generator_forward(
                    noisy_image_or_video=denoised_pred,
                    conditional_dict=block_conditional_dict,
                    timestep=context_timestep,
                    kv_cache=self.causal_pipe.kv_cache1,
                    crossattn_cache=self.causal_pipe.crossattn_cache,
                    current_start=current_start_frame * self.causal_pipe.frame_seq_length,
                )
                context_time = time.time() - context_start_time
                self._record_timing("denoising_context_update", context_time, chunk_id)
                
                # è®°å½•æ•´ä¸ªdenoisingä»»åŠ¡çš„æ€»æ—¶é—´
                total_denoising_time = time.time() - denoising_start_time
                self._record_timing("denoising_total", total_denoising_time, chunk_id)
                
                # è®°å½•æ•´ä¸ªdenoisingä»»åŠ¡çš„æ€»æ—¶é—´
                total_denoising_time = time.time() - denoising_start_time
                self._record_timing("denoising_total", total_denoising_time, chunk_id)

                # Trigger denoising event - use chunk_id instead of self.current_clock to avoid race condition
                self.denoising_events[chunk_id].set()
                
                # Create VAE event for this block
                self.vae_events.append(threading.Event())
                
                # Add VAE decoding task (video is already decoded by causal_pipe.inference)
                # But we still need to process it for streaming
                # if current_start_frame > 0:
                #     task.update({
                #         "latents": output[:, current_start_frame - 1:current_start_frame + current_num_frames].clone()
                #     })
                # else:
                # if current_start_frame > 0:
                #     latents = output[:, current_start_frame-1:current_start_frame + current_num_frames]
                # else:
                latents = output[:, current_start_frame:current_start_frame + current_num_frames]
                print(f"chunk_id: {chunk_id}, latents: {latents.shape}")
                task.update({
                    "latents": latents.clone()
                })
                self.vae_queue.put(task)
                
                print(f"Causal denoising inference completed for block {chunk_id}")
                self.denoising_queue.task_done()

            except queue.Empty:
                # Check if we should stop
                if self.stop_workers.is_set():
                    print("Denoising worker received stop signal")
                    break
                pass
            except Exception as e:
                print(f"Error in causal denoising inference for block {chunk_id}: {e}")
                traceback.print_exc()
                self.denoising_queue.task_done()
    
    @torch.no_grad()
    def _vae_worker(self):
        """
        VAE processing worker (mainly for format conversion and streaming)
        """
        while True:
            try:
                task = self.vae_queue.get(timeout=0.1)
                chunk_id = task["chunk_id"]
                print(f"Processing video formatting for block {chunk_id}")
                
                # è®°å½•æ•´ä¸ªVAEä»»åŠ¡çš„å¼€å§‹æ—¶é—´
                vae_start_time = time.time()
                
                latents: torch.Tensor = task["latents"]
                
                # æ•°æ®å‡†å¤‡é˜¶æ®µ
                prep_start_time = time.time()
                latents = latents.permute(0, 2, 1, 3, 4)    # (b, c, t, h, w)
                latents = latents.to("cuda:1")
                prep_time = time.time() - prep_start_time
                self._record_timing("vae_data_preparation", prep_time, chunk_id)
                
                # VAEè§£ç é˜¶æ®µ
                decode_start_time = time.time()
                video = self.causal_pipe.vae.decode(latents, device="cuda:1", tiled=False, tile_size=(30, 52), tile_stride=(15, 26)).permute(0, 2, 1, 3, 4)
                decode_time = time.time() - decode_start_time
                self._record_timing("vae_decode", decode_time, chunk_id)
                
                # åå¤„ç†é˜¶æ®µ
                postprocess_start_time = time.time()
                # video = video[:, :, 1:].permute(0, 2, 1, 3, 4)
                # video = video[:, :, 1:]
                # Ensure video is in float32 for compatibility with numpy conversion
                # video = video[:, 1:]
                print(f"Video before float conversion - dtype: {video.dtype}, shape: {video.shape}")
                video = (video.float() + 1) / 2  # Normalize from [-1, 1] to [0, 1]
                print(f"Video after float conversion - dtype: {video.dtype}, shape: {video.shape}")
                postprocess_time = time.time() - postprocess_start_time
                self._record_timing("vae_postprocessing", postprocess_time, chunk_id)
                
                # Store result in buffer with lock
                buffer_start_time = time.time()
                task.update({"video": video})
                with self.result_lock:
                    self.result_buffer[chunk_id] = task
                buffer_time = time.time() - buffer_start_time
                self._record_timing("vae_buffer_storage", buffer_time, chunk_id)
                
                # è®°å½•æ•´ä¸ªVAEä»»åŠ¡çš„æ€»æ—¶é—´
                total_vae_time = time.time() - vae_start_time
                self._record_timing("vae_total", total_vae_time, chunk_id)
                
                print(f"Video formatting completed for block {chunk_id}")
                # trigger vae event - use chunk_id instead of self.current_clock - 1 to avoid race condition
                self.vae_events[chunk_id].set()
                self.vae_queue.task_done()
            except queue.Empty:
                # Check if we should stop
                if self.stop_workers.is_set():
                    print("VAE worker received stop signal")
                    break
                pass
            except Exception as e:
                print(f"Error in video formatting for block {chunk_id}: {e}")
                traceback.print_exc()
                self.vae_queue.task_done()

    def _send_chunk_frames(self, chunk_id, streaming_callback, session_id, total_frames_generated, total_blocks):
        if chunk_id < 0:
            return 0
        """Send frames for a completed block using frame-by-frame method"""
        print(f"Sending block {chunk_id} frames")
        # import pdb; pdb.set_trace()
        # è®°å½•å‘é€å¸§çš„å¼€å§‹æ—¶é—´
        send_start_time = time.time()
        
        # Wait for result to be available
        wait_start_time = time.time()
        while chunk_id not in self.result_buffer:
            time.sleep(0.1)
        wait_time = time.time() - wait_start_time
        self._record_timing("send_wait_for_result", wait_time, chunk_id)
        
        block_data = self.result_buffer[chunk_id]
        video = block_data["video"]
        
        # audio
        audio_segment = block_data["audio_segment"]
        
        # ç›´æ¥ä½¿ç”¨é€å¸§å‘é€æ–¹å¼ï¼Œé¿å…è§†é¢‘æµåŒæ­¥é—®é¢˜
        print(f"Using frame-by-frame method for chunk {chunk_id}")
        frame_send_start_time = time.time()
        total_frames_generated = self._send_frames_fallback(
            video, audio_segment, chunk_id, streaming_callback, 
            session_id, total_frames_generated, total_blocks
        )
        frame_send_time = time.time() - frame_send_start_time
        self._record_timing("send_frames_processing", frame_send_time, chunk_id)
        
        # è®°å½•æ•´ä¸ªå‘é€è¿‡ç¨‹çš„æ€»æ—¶é—´
        total_send_time = time.time() - send_start_time
        self._record_timing("send_total", total_send_time, chunk_id)
        print(f"total_frames_generated: {total_frames_generated}")
        
        # return total_frames_generated + video.shape[1]
        return total_frames_generated

    def _send_frames_fallback(self, video, audio_segment, chunk_id, streaming_callback, session_id, total_frames_generated, total_blocks):
        """Frame-by-frame sending method (following pipelined_inference.py pattern)"""
        print(f"Using frame-by-frame method for chunk {chunk_id}")
        # import pdb; pdb.set_trace()
        total_frames = (total_blocks * self.args.num_frame_per_block - 1) * 4 + 1
        # import pdb; pdb.set_trace()
        frame_duration = 1.0 / self.args.fps
        audio_sample_rate = 24000  # éŸ³é¢‘é‡‡æ ·ç‡
        samples_per_frame = int(audio_sample_rate / self.args.fps)  # æ¯å¸§å¯¹åº”çš„é‡‡æ ·ç‚¹æ•°
        frame_processing_times = []
        for frame_idx in range(video.shape[1]):
            frame_start_time = time.time()
            
            frame_data = video[:, frame_idx]
            
            # å›¾åƒè½¬æ¢æ—¶é—´
            convert_start_time = time.time()
            frame_np = (frame_data.squeeze(0).permute(1, 2, 0).cpu().float().numpy() * 255).astype('uint8')
            frame_pil = Image.fromarray(frame_np)
            buffer = io.BytesIO()
            frame_pil.save(buffer, format='JPEG', quality=85)
            frame_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')
            convert_time = time.time() - convert_start_time
            
            # audio clip - ä½¿ç”¨é‡‡æ ·ç‚¹ç´¢å¼•è€Œä¸æ˜¯æ—¶é—´
            audio_start_sample = int(frame_idx * samples_per_frame)
            audio_end_sample = int(audio_start_sample + samples_per_frame)
            
            # è¾¹ç•Œæ£€æŸ¥ï¼šç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            audio_start_sample = max(0, audio_start_sample)
            audio_end_sample = min(len(audio_segment), audio_end_sample)
            
            audio_start_time = frame_idx * frame_duration  # ä¿ç•™æ—¶é—´ä¿¡æ¯ç”¨äºå›è°ƒ
            audio_end_time = audio_start_time + frame_duration
            
            # æå–éŸ³é¢‘ç‰‡æ®µ
            if audio_start_sample >= audio_end_sample:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆéŸ³é¢‘ï¼Œåˆ›å»ºé™éŸ³
                current_frame_audio = np.zeros(samples_per_frame, dtype=np.int16)
                current_frame_audio_base64 = None
                print(f"Warning: No valid audio for frame {frame_idx}, using silence")
            else:
                current_frame_audio = audio_segment[audio_start_sample:audio_end_sample]
                
                # å¦‚æœéŸ³é¢‘æ˜¯ç«‹ä½“å£°ï¼Œè½¬æ¢ä¸ºå•å£°é“
                if len(current_frame_audio.shape) > 1:
                    current_frame_audio = np.mean(current_frame_audio, axis=1)
                
                # è½¬æ¢ä¸º16ä½æ•´æ•°
                current_frame_audio = (current_frame_audio * 32767).astype(np.int16)
                
                # å†™å…¥ä¸´æ—¶æ–‡ä»¶å¹¶è½¬æ¢ä¸ºbase64
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    sf.write(temp_file.name, current_frame_audio, audio_sample_rate)
                    with open(temp_file.name, 'rb') as f:
                        current_frame_audio_base64 = base64.b64encode(f.read()).decode('utf-8')
                    os.unlink(temp_file.name)
            
            total_frames_generated += 1
            progress_overall = (total_frames_generated / total_frames) * 100 if total_frames else None
            
            # å›è°ƒå‘é€æ—¶é—´
            callback_start_time = time.time()
            # import pdb; pdb.set_trace()
            streaming_callback({
                "type": "video_frame",
                "session_id": session_id,
                "frame_data": frame_base64,
                "frame_number": total_frames_generated,
                "total_frames": total_frames,
                "chunk_number": chunk_id,
                "progress": progress_overall,
                "chunk_progress": ((frame_idx + 1) / video.shape[1]) * 100,
                "audio_segment": current_frame_audio_base64,
                "audio_start_time": audio_start_time,
                "audio_duration": frame_duration
            })
            callback_time = time.time() - callback_start_time
            
            frame_total_time = time.time() - frame_start_time
            frame_processing_times.append({
                'convert_time': convert_time,
                'callback_time': callback_time,
                'total_time': frame_total_time
            })
        
        # è®°å½•å¸§å¤„ç†çš„å¹³å‡æ—¶é—´
        if frame_processing_times:
            avg_convert_time = np.mean([t['convert_time'] for t in frame_processing_times])
            avg_callback_time = np.mean([t['callback_time'] for t in frame_processing_times])
            avg_frame_time = np.mean([t['total_time'] for t in frame_processing_times])
            
            self._record_timing("send_frame_convert_avg", avg_convert_time, chunk_id)
            self._record_timing("send_frame_callback_avg", avg_callback_time, chunk_id)
            self._record_timing("send_frame_total_avg", avg_frame_time, chunk_id)
        
        # Chunk complete (same format as pipelined_inference.py)
        progress_overall_after_chunk = (total_frames_generated / total_frames) * 100 if total_frames else None
        streaming_callback({
            "type": "chunk_complete",
            "session_id": session_id,
            "chunk_number": chunk_id,
            "total_chunks": total_blocks,
            "frames_in_chunk": video.shape[1],
            "total_frames_generated": total_frames_generated,
            "progress": progress_overall_after_chunk,
            "message": f"Chunk {chunk_id} completed (causal pipelined)"
        })
        
        return total_frames_generated

    def _save_complete_video_with_audio(self, num_blocks: int, audio_path: Optional[str], session_id: Optional[str]):
        """
        ä¿å­˜å®Œæ•´è§†é¢‘å¹¶ä¸éŸ³é¢‘åˆå¹¶
        
        Args:
            num_blocks: æ€»çš„å—æ•°
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            session_id: ä¼šè¯IDï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
        """
        print("\n" + "="*80)
        print("SAVING COMPLETE VIDEO WITH AUDIO")
        print("="*80)
        
        save_start_time = time.time()
        
        # æ­¥éª¤1: ä»result_bufferæ”¶é›†æ‰€æœ‰è§†é¢‘å—
        print(f"æ”¶é›† {num_blocks} ä¸ªè§†é¢‘å—...")
        all_videos = []
        for chunk_id in range(num_blocks):
            if chunk_id not in self.result_buffer:
                print(f"è­¦å‘Š: chunk {chunk_id} ä¸åœ¨ result_buffer ä¸­")
                continue
            
            video_chunk = self.result_buffer[chunk_id]["video"]
            all_videos.append(video_chunk)
            print(f"  Chunk {chunk_id}: shape {video_chunk.shape}")
        
        if len(all_videos) == 0:
            print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘å—!")
            return
        
        # æ­¥éª¤2: æ‹¼æ¥æ‰€æœ‰è§†é¢‘å—
        print(f"æ‹¼æ¥ {len(all_videos)} ä¸ªè§†é¢‘å—...")
        complete_video = torch.cat(all_videos, dim=1)  # åœ¨æ—¶é—´ç»´åº¦æ‹¼æ¥
        print(f"å®Œæ•´è§†é¢‘ shape: {complete_video.shape}")
        
        # æ­¥éª¤3: è½¬æ¢ä¸ºnumpyæ•°ç»„
        print("å°†è§†é¢‘è½¬æ¢ä¸º numpy æ•°ç»„...")
        # Shape: (batch, time, channels, height, width) -> (time, height, width, channels)
        video_np = complete_video.squeeze(0).permute(0, 2, 3, 1).cpu().float().numpy()
        video_np = (video_np * 255).astype(np.uint8)
        print(f"è§†é¢‘ numpy shape: {video_np.shape}")
        
        # æ­¥éª¤4: åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = "output_videos"
        os.makedirs(output_dir, exist_ok=True)
        
        # æ­¥éª¤5: ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_str = f"_{session_id}" if session_id else ""
        video_no_audio_path = os.path.join(output_dir, f"video_no_audio{session_str}_{timestamp}.mp4")
        video_with_audio_path = os.path.join(output_dir, f"video_with_audio{session_str}_{timestamp}.mp4")
        
        # æ­¥éª¤6: ä¿å­˜æ— éŸ³é¢‘è§†é¢‘
        print(f"ä¿å­˜æ— éŸ³é¢‘è§†é¢‘åˆ°: {video_no_audio_path}")
        fps = getattr(self.args, 'fps', 16)
        height, width = video_np.shape[1:3]
        
        # ä½¿ç”¨cv2ä¿å­˜è§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        video_writer = cv2.VideoWriter(video_no_audio_path, fourcc, fps, (width, height))
        
        for frame_idx in range(video_np.shape[0]):
            frame_bgr = cv2.cvtColor(video_np[frame_idx], cv2.COLOR_RGB2BGR)
            video_writer.write(frame_bgr)
        
        video_writer.release()
        video_save_time = time.time() - save_start_time
        print(f"âœ“ æ— éŸ³é¢‘è§†é¢‘ä¿å­˜æˆåŠŸ (ç”¨æ—¶ {video_save_time:.2f}s)")
        
        # æ­¥éª¤7: å¦‚æœæœ‰éŸ³é¢‘è·¯å¾„ï¼Œä½¿ç”¨ffmpegåˆå¹¶éŸ³é¢‘
        if audio_path is not None and os.path.exists(audio_path):
            print(f"\nä½¿ç”¨ ffmpeg åˆå¹¶éŸ³é¢‘: {audio_path}")
            merge_start_time = time.time()
            
            try:
                # æ„å»ºffmpegå‘½ä»¤
                ffmpeg_cmd = [
                    'ffmpeg',
                    '-i', video_no_audio_path,  # è¾“å…¥è§†é¢‘
                    '-i', audio_path,            # è¾“å…¥éŸ³é¢‘
                    '-map', '0:v',               # é€‰æ‹©ç¬¬ä¸€ä¸ªè¾“å…¥çš„è§†é¢‘æµ
                    '-map', '1:a',               # é€‰æ‹©ç¬¬äºŒä¸ªè¾“å…¥çš„éŸ³é¢‘æµ
                    '-c:v', 'libx264',           # è§†é¢‘ç¼–ç å™¨
                    '-preset', 'medium',         # ç¼–ç é¢„è®¾
                    '-crf', '23',                # è´¨é‡å‚æ•°
                    '-c:a', 'aac',               # éŸ³é¢‘ç¼–ç å™¨
                    '-b:a', '192k',              # éŸ³é¢‘æ¯”ç‰¹ç‡
                    '-shortest',                 # ä»¥æœ€çŸ­çš„æµä¸ºå‡†
                    '-y',                        # è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
                    video_with_audio_path
                ]
                
                print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(ffmpeg_cmd)}")
                result = subprocess.run(
                    ffmpeg_cmd,
                    capture_output=True,
                    text=True,
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                
                if result.returncode == 0:
                    merge_time = time.time() - merge_start_time
                    print(f"âœ“ éŸ³ç”»åˆå¹¶è§†é¢‘ä¿å­˜æˆåŠŸ: {video_with_audio_path}")
                    print(f"  éŸ³é¢‘åˆå¹¶ç”¨æ—¶: {merge_time:.2f}s")
                else:
                    print(f"âœ— ffmpeg é”™è¯¯ (è¿”å›ç : {result.returncode})")
                    print(f"  stderr: {result.stderr}")
                    print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
                    
            except subprocess.TimeoutExpired:
                print("âœ— ffmpeg è¶…æ—¶ (è¶…è¿‡5åˆ†é’Ÿ)")
                print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
            except FileNotFoundError:
                print("âœ— æœªæ‰¾åˆ° ffmpeg å‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£… ffmpeg")
                print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
            except Exception as e:
                print(f"âœ— éŸ³é¢‘åˆå¹¶å¼‚å¸¸: {e}")
                traceback.print_exc()
                print(f"  æ— éŸ³é¢‘è§†é¢‘ä»å¯ç”¨: {video_no_audio_path}")
        else:
            if audio_path is None:
                print("\næœªæä¾›éŸ³é¢‘è·¯å¾„ï¼Œè·³è¿‡éŸ³é¢‘åˆå¹¶")
            else:
                print(f"\néŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}ï¼Œè·³è¿‡éŸ³é¢‘åˆå¹¶")
        
        total_save_time = time.time() - save_start_time
        print(f"\næ€»ä¿å­˜æ—¶é—´: {total_save_time:.2f}s")
        print("="*80 + "\n")
